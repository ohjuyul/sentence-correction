# data_txt.py
import os
from typing import Dict, List, Optional, Iterator

import torch
from torch.utils.data import Dataset
from transformers import BertTokenizer, PreTrainedTokenizerBase


# ------------------------------
# 유틸 함수
# ------------------------------
def ensure_special_tokens(tok: PreTrainedTokenizerBase) -> PreTrainedTokenizerBase:
    """
    디코더 토크나이저의 PAD/BOS/EOS 토큰을 보장합니다.
    """
    if tok.pad_token is None:
        tok.add_special_tokens({"pad_token": "<pad>"})
    # 일부 토크나이저는 bos_token 속성이 없을 수 있음
    if getattr(tok, "bos_token", None) is None:
        tok.add_special_tokens({"bos_token": "<s>"})
    if tok.eos_token is None:
        tok.add_special_tokens({"eos_token": "</s>"})
    return tok


def shift_right(labels: torch.LongTensor, bos_id: int) -> torch.LongTensor:
    """
    라벨을 기준으로 디코더 입력을 생성합니다.
    decoder_input_ids = [BOS] + labels[:-1]
    (주의) labels 내 -100 은 단지 loss mask 용도로만 쓰이며, 실제 토큰 시퀀스는 원본 ids로 구성되어야 합니다.
    """
    assert labels.dim() == 2, "labels must be (B, L)"
    # -100 은 임시로 0으로 치환해도 디코더 입력 토큰 id 범위 체크에 영향이 없도록 처리
    base = labels.clone()
    base = base.masked_fill(base.eq(-100), 0)
    bos_col = torch.full((base.size(0), 1), bos_id, dtype=base.dtype, device=base.device)
    return torch.cat([bos_col, base[:, :-1]], dim=1)


# ------------------------------
# Dataset 본체
# ------------------------------
class GrammarCorrectionDataset(Dataset):
    """
    - 인코더: KC-BERT 토크나이저(입력 원문)
    - 디코더: 사용자 SPM(T5TokenizerFast 등) 토크나이저(정답 문장)
    - 출력 키:
        input_ids, attention_mask, labels, decoder_input_ids, decoder_attention_mask
    """
    def __init__(
        self,
        ko_file: str,
        correct_file: str,
        tokenizer_name: str = "beomi/kcbert-base",
        max_length: int = 300,
        stride: int = 64,
        limit: Optional[int] = None,
        decoder_tokenizer: Optional[PreTrainedTokenizerBase] = None,
        dec_max_length: int = 300,
    ):
        super().__init__()
        # 인코더(소스) 토크나이저
        self.src_tok: BertTokenizer = BertTokenizer.from_pretrained(tokenizer_name)
        self.max_length = max_length
        self.dec_max_length = dec_max_length
        self.stride = stride
        self.limit = limit

        # 디코더(타깃) 토크나이저
        if decoder_tokenizer is None:
            raise ValueError("decoder_tokenizer를 반드시 전달해야 합니다.")
        self.dec_tok: PreTrainedTokenizerBase = ensure_special_tokens(decoder_tokenizer)

        self.examples: List[Dict[str, torch.Tensor]] = []

        # 파일 읽기
        with open(ko_file, "r", encoding="utf-8") as f_ko, open(correct_file, "r", encoding="utf-8") as f_correct:
            originals = f_ko.readlines()
            corrected = f_correct.readlines()

        assert len(originals) == len(corrected), "원본과 정답 파일의 문장 수가 다릅니다."

        # 문장 단위로 순회
        for idx, (src, tgt) in enumerate(zip(originals, corrected)):
            if self.limit is not None and idx >= self.limit:
                break

            src = src.strip()
            tgt = tgt.strip()
            if not src or not tgt:
                continue

            # 1) 소스: 슬라이딩 윈도우 토크나이즈 (인코더 입력용)
            src_chunks = self._tokenize_source(src)

            # 2) 타깃: 디코더 토큰화(문장 단위), labels/decoder_input_ids/decoder_attention_mask 생성
            tgt_pack = self._tokenize_target(tgt)
            labels: torch.Tensor = tgt_pack["labels"]
            decoder_input_ids: torch.Tensor = tgt_pack["decoder_input_ids"]
            decoder_attention_mask: torch.Tensor = tgt_pack["decoder_attention_mask"]

            # 3) 슬라이싱된 소스 chunk 각각에 동일 타깃을 매핑 (가장 안전한 정렬 방식)
            n = src_chunks["input_ids"].size(0)
            for i in range(n):
                self.examples.append(
                    {
                        "input_ids": src_chunks["input_ids"][i],                 # (L,)
                        "attention_mask": src_chunks["attention_mask"][i],       # (L,)
                        "labels": labels,                                        # (L,)
                        "decoder_input_ids": decoder_input_ids,                  # (L,)
                        "decoder_attention_mask": decoder_attention_mask,        # (L,)
                    }
                )

    # ------------------------------
    # 보조 토크나이즈 함수 (DRY)
    # ------------------------------
    def _tokenize_source(self, text: str) -> Dict[str, torch.Tensor]:
        """
        인코더 입력(소스)을 슬라이딩 윈도우로 토크나이즈합니다.
        padding='max_length' 로 고정(모델/훈련 파이프라인 정책에 맞춰 필요시 조정)
        """
        return self.src_tok(
            text,
            max_length=self.max_length,
            truncation=True,
            padding="max_length",
            stride=self.stride,
            return_overflowing_tokens=True,
            return_tensors="pt",
        )

    def _tokenize_target(self, text: str) -> Dict[str, torch.Tensor]:
        """
        디코더 입력/라벨을 생성합니다.
        - labels: tgt ids (PAD → -100)
        - decoder_input_ids: shift-right(labels)
        - decoder_attention_mask: (decoder_input_ids != pad_id)
        """
        enc = self.dec_tok(
            text,
            max_length=self.dec_max_length,
            truncation=True,
            padding="max_length",
            return_tensors="pt",
        )
        # (1, L) → (L,)
        tgt_ids: torch.Tensor = enc["input_ids"][0]
        pad_id = self.dec_tok.pad_token_id
        bos_id = getattr(self.dec_tok, "bos_token_id", None)
        if bos_id is None:
            raise ValueError("디코더 토크나이저에 bos_token_id가 필요합니다.")

        # labels: pad → -100
        labels = tgt_ids.clone()
        labels[labels == pad_id] = -100

        # decoder_input_ids: shift-right(labels 기반)
        dec_inp = shift_right(labels.unsqueeze(0), bos_id=bos_id)[0]

        # decoder_attention_mask
        dec_attn = (dec_inp != pad_id).long()

        return {
            "labels": labels,                           # (L,)
            "decoder_input_ids": dec_inp,               # (L,)
            "decoder_attention_mask": dec_attn,         # (L,)
        }

    # ------------------------------
    # 표준 Dataset 인터페이스
    # ------------------------------
    def __len__(self) -> int:
        return len(self.examples)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        return self.examples[idx]

    # ------------------------------
    # 편의: 배치 제너레이터
    # ------------------------------
    def get_batches(self, batch_size: int = 8, device: Optional[str] = None) -> Iterator[Dict[str, torch.Tensor]]:
        """
        간단한 배치 제너레이터 (학습 루프에서 DataLoader 대신 빠르게 확인할 때 사용)
        """
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"

        for i in range(0, len(self.examples), batch_size):
            batch = self.examples[i : i + batch_size]
            # key별 스택 후 디바이스 이동
            batch_tensors = {k: torch.stack([item[k] for item in batch]).to(device) for k in batch[0]}
            yield batch_tensors

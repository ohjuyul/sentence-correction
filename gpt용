import os
from torch.utils.data import DataLoader
from model3 import CorrectionModel  # 모델 임포트
from data_txt import GrammarCorrectionDataset  # Dataset 임포트
from transformers import T5TokenizerFast
import torch

# 디바이스 설정: CUDA (GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 디코더 토크나이저 로딩
dec_tok = T5TokenizerFast(vocab_file="../spm_out/tokenizer_32k.model", extra_ids=0)

# 모델 로딩
model = CorrectionModel(
    encoder_name="beomi/kcbert-base",
    num_decoder_layers=6,
    decoder_tokenizer=dec_tok,  # decoder_tokenizer 전달
)

# 모델을 GPU로 이동
model.to(device)

# 데이터셋 로딩 (여기서 limit을 주면 데이터 크기를 줄일 수 있음)
dataset = GrammarCorrectionDataset(ko_file="../ko.txt", correct_file="../correct.txt", tokenizer_name="beomi/kcbert-base", limit=100000)
train_loader = DataLoader(dataset, batch_size=8, shuffle=True)

# 배치 테스트 (배치 하나를 GPU로 이동)
for batch in train_loader:
    # 배치가 너무 크지 않도록 CPU에서 GPU로 배치 크기를 적절히 이동
    batch = {key: value.to(device) for key, value in batch.items()}  # 배치를 GPU로 이동
    
    # 배치 확인
    print(batch)  

    # 모델-디코더 토크나이저 동기화 확인
    assert model.config.pad_token_id == model.dec_tok.pad_token_id
    assert model.config.eos_token_id == model.dec_tok.eos_token_id
    assert model.config.decoder_start_token_id == getattr(model.dec_tok, "bos_token_id", None)

    # 출력 차원 vs Vocab 크기 일치 확인
    assert model.output_projection.out_features == model.dec_tok.vocab_size

    # 배치 검사 (DataLoader에서 하나 뽑아서 확인)
    assert batch["labels"].max().item() < model.dec_tok.vocab_size
    assert batch["decoder_input_ids"][:, 0].eq(model.dec_tok.bos_token_id).all()
    assert set(batch["decoder_attention_mask"].unique().tolist()) <= {0, 1}

    print("모델과 디코더 토크나이저의 동기화가 제대로 되어 있습니다!")
    break  # 한 배치만 테스트

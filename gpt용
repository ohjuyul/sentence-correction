# data_txt.py
import os
from typing import Dict, List, Optional, Iterator

import torch
from torch.utils.data import Dataset
from transformers import BertTokenizer, PreTrainedTokenizerBase

IGNORE_INDEX = -100


# ------------------------------
# 유틸 함수
# ------------------------------
def ensure_special_tokens(tok: PreTrainedTokenizerBase) -> PreTrainedTokenizerBase:
    """
    디코더 토크나이저의 PAD/BOS/EOS 토큰을 보장합니다.
    모델 쪽 임베딩 리사이즈는 모델 초기화 시 처리(데이터셋은 토크나이저만 보증).
    """
    if tok.pad_token is None:
        tok.add_special_tokens({"pad_token": "<pad>"})
    if getattr(tok, "bos_token", None) is None:
        tok.add_special_tokens({"bos_token": "<s>"})
    if tok.eos_token is None:
        tok.add_special_tokens({"eos_token": "</s>"})
    return tok


def shift_right(labels: torch.LongTensor, bos_id: int) -> torch.LongTensor:
    """
    (B, L) 라벨을 기준으로 디코더 입력을 생성합니다.
    decoder_input_ids = [BOS] + labels_without_ignore[:, :-1]
    - labels 내 IGNORE_INDEX(-100)는 디코더 입력 생성 시 0으로 치환(토큰 의미 없음)
    """
    assert labels.dim() == 2, "labels must be (B, L)"
    base = labels.masked_fill(labels.eq(IGNORE_INDEX), 0)
    bos_col = torch.full((base.size(0), 1), bos_id, dtype=base.dtype, device=base.device)
    return torch.cat([bos_col, base[:, :-1]], dim=1)


# ------------------------------
# Dataset 본체
# ------------------------------
class GrammarCorrectionDataset(Dataset):
    """
    한국어 문맥교정용 Dataset
    - 인코더: KC-BERT 토크나이저(입력 원문)
    - 디코더: 사용자 SPM(T5Tokenizer/Fast 등) 토크나이저(정답 문장)
    - 출력 키:
        input_ids, attention_mask, labels, decoder_input_ids, decoder_attention_mask
    사용 예:
        dataset = GrammarCorrectionDataset(
            ko_file="ko.txt",
            correct_file="correct.txt",
            tokenizer_name="beomi/kcbert-base",
            max_length=300,
            stride=64,
            limit=None,
            decoder_tokenizer=dec_tok,
            dec_max_length=512,
        )
    """
    def __init__(
        self,
        ko_file: str,
        correct_file: str,
        tokenizer_name: str = "beomi/kcbert-base",
        max_length: int = 300,
        stride: int = 64,
        limit: Optional[int] = None,
        decoder_tokenizer: Optional[PreTrainedTokenizerBase] = None,
        dec_max_length: int = 512,
    ):
        super().__init__()
        # 인코더(소스) 토크나이저
        self.src_tok: BertTokenizer = BertTokenizer.from_pretrained(tokenizer_name)
        self.max_length = max_length
        self.dec_max_length = dec_max_length
        self.stride = stride
        self.limit = limit

        # 디코더(타깃) 토크나이저
        if decoder_tokenizer is None:
            raise ValueError("decoder_tokenizer를 반드시 전달해야 합니다.")
        self.dec_tok: PreTrainedTokenizerBase = ensure_special_tokens(decoder_tokenizer)

        # 필수 특수 토큰 ID 확인
        if getattr(self.dec_tok, "bos_token_id", None) is None:
            raise ValueError("디코더 토크나이저에 bos_token_id가 필요합니다.")
        if self.dec_tok.pad_token_id is None or self.dec_tok.eos_token_id is None:
            raise ValueError("디코더 토크나이저의 pad/eos 토큰이 필요합니다.")

        self.examples: List[Dict[str, torch.Tensor]] = []

        # 파일 존재 확인 및 로드
        if not (os.path.exists(ko_file) and os.path.exists(correct_file)):
            raise FileNotFoundError(f"ko_file or correct_file not found: {ko_file}, {correct_file}")

        with open(ko_file, "r", encoding="utf-8") as f_ko, open(correct_file, "r", encoding="utf-8") as f_correct:
            originals = f_ko.readlines()
            corrected = f_correct.readlines()

        if len(originals) != len(corrected):
            raise AssertionError(f"원본/정답 라인 수 불일치: {len(originals)} vs {len(corrected)}")

        # 문장 단위로 순회
        for idx, (src, tgt) in enumerate(zip(originals, corrected)):
            if self.limit is not None and idx >= self.limit:
                break

            src = (src or "").strip()
            tgt = (tgt or "").strip()
            if not src or not tgt:
                continue

            # 1) 소스: 슬라이딩 윈도우 토크나이즈 (인코더 입력용)
            src_chunks = self._tokenize_source(src)

            # 2) 타깃: 디코더 토큰화(문장 단위), labels/decoder_input_ids/decoder_attention_mask 생성
            tgt_pack = self._tokenize_target(tgt)
            labels: torch.Tensor = tgt_pack["labels"]                       # (L,)
            decoder_input_ids: torch.Tensor = tgt_pack["decoder_input_ids"] # (L,)
            decoder_attention_mask: torch.Tensor = tgt_pack["decoder_attention_mask"]  # (L,)

            # 3) 슬라이싱된 소스 chunk 각각에 동일 타깃을 매핑 (정렬 안정)
            n = src_chunks["input_ids"].size(0)
            for i in range(n):
                self.examples.append(
                    {
                        "input_ids": src_chunks["input_ids"][i].clone(),                 # (L,)
                        "attention_mask": src_chunks["attention_mask"][i].clone(),       # (L,)
                        "labels": labels.clone(),                                        # (L,)
                        "decoder_input_ids": decoder_input_ids.clone(),                  # (L,)
                        "decoder_attention_mask": decoder_attention_mask.clone(),        # (L,)
                    }
                )

    # ------------------------------
    # 보조 토크나이즈 함수 (DRY)
    # ------------------------------
    def _tokenize_source(self, text: str) -> Dict[str, torch.Tensor]:
        """
        인코더 입력(소스)을 슬라이딩 윈도우로 토크나이즈합니다.
        padding='max_length' 고정(훈련/추론 정책에 맞춰 필요 시 조정).
        """
        return self.src_tok(
            text,
            max_length=self.max_length,
            truncation=True,
            padding="max_length",
            stride=self.stride,
            return_overflowing_tokens=True,
            return_tensors="pt",
        )

    def _tokenize_target(self, text: str) -> Dict[str, torch.Tensor]:
        """
        디코더 입력/라벨을 생성합니다.
        - labels: tgt ids (PAD → -100)
        - decoder_input_ids: shift-right(labels 기반, BOS 선행)
        - decoder_attention_mask: (decoder_input_ids != pad_id)
        """
        enc = self.dec_tok(
            text,
            max_length=self.dec_max_length,
            truncation=True,
            padding="max_length",
            return_tensors="pt",
        )
        # (1, L) → (L,)
        tgt_ids: torch.Tensor = enc["input_ids"][0]
        pad_id = self.dec_tok.pad_token_id
        bos_id = self.dec_tok.bos_token_id

        # labels: pad → -100 (loss에서 무시)
        labels = tgt_ids.clone()
        labels[labels == pad_id] = IGNORE_INDEX

        # decoder_input_ids: shift-right
        dec_inp = shift_right(labels.unsqueeze(0), bos_id=bos_id)[0]

        # decoder_attention_mask: 디코더 입력에서 pad가 아닌 곳
        dec_attn = (dec_inp != pad_id).long()

        return {
            "labels": labels,                           # (L,)
            "decoder_input_ids": dec_inp,               # (L,)
            "decoder_attention_mask": dec_attn,         # (L,)
        }

    # ------------------------------
    # 표준 Dataset 인터페이스
    # ------------------------------
    def __len__(self) -> int:
        return len(self.examples)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        return self.examples[idx]

    # ------------------------------
    # 편의: 배치 제너레이터(선택)
    # ------------------------------
    def get_batches(self, batch_size: int = 8, device: Optional[str] = None) -> Iterator[Dict[str, torch.Tensor]]:
        """
        간단한 배치 제너레이터 (DataLoader 없이 빠른 확인용)
        """
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"

        for i in range(0, len(self.examples), batch_size):
            batch = self.examples[i: i + batch_size]
            batch_tensors = {k: torch.stack([item[k] for item in batch]).to(device) for k in batch[0]}
            yield batch_tensors

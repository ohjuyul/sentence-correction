# train_sanity.py
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from typing import Dict, Iterable, Tuple
import math
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import T5TokenizerFast, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup

from model3 import CorrectionModel
from data_txt import GrammarCorrectionDataset


# -----------------------------
# 구성
# -----------------------------
ENCODER_NAME = "beomi/kcbert-base"
DECODER_SPM_PATH = "../spm_out/tokenizer_32k.model"
KO_FILE = "../ko.txt"
CORRECT_FILE = "../correct.txt"

# 길이: 인코더/디코더 분리 (KC-BERT enc<=300)
ENC_MAX_LENGTH = 300
DEC_MAX_LENGTH = 512
STRIDE = 64

# 데이터/학습 파라미터
BATCH_SIZE = 8
NUM_WORKERS = max(1, (os.cpu_count() or 2) - 1)
LR = 3e-4
WEIGHT_DECAY = 0.01
NUM_WARMUP_STEPS = 100
TOTAL_TRAIN_STEPS = 1000      # 미니 검증이니 작게
USE_COSINE = True             # False면 linear
GRAD_CLIP_NORM = 1.0
LABEL_SMOOTHING = 0.1         # 0.0~0.1 권장
ACCUM_STEPS = 1               # 필요시 2~4로 늘려도 됨
SANITY_BATCHES = 200          # 몇 배치만 확인


# -----------------------------
# 토크나이저
# -----------------------------
def ensure_special_tokens(tok: T5TokenizerFast) -> T5TokenizerFast:
    changed = False
    if tok.pad_token is None:
        tok.add_special_tokens({"pad_token": "<pad>"}); changed = True
    if getattr(tok, "bos_token", None) is None:
        tok.add_special_tokens({"bos_token": "<s>"}); changed = True
    if tok.eos_token is None:
        tok.add_special_tokens({"eos_token": "</s>"}); changed = True
    if changed:
        print("[Info] Added missing special tokens to decoder tokenizer.")
    return tok


def build_decoder_tokenizer(spm_path: str, legacy: bool = False) -> T5TokenizerFast:
    if not os.path.exists(spm_path):
        raise FileNotFoundError(f"Decoder SPM not found: {spm_path}")
    tok = T5TokenizerFast(vocab_file=spm_path, extra_ids=0, legacy=legacy)
    return ensure_special_tokens(tok)


# -----------------------------
# 데이터
# -----------------------------
def build_loader(
    dec_tok: T5TokenizerFast,
    batch_size: int,
    num_workers: int,
) -> DataLoader:
    dataset = GrammarCorrectionDataset(
        ko_file=KO_FILE,
        correct_file=CORRECT_FILE,
        tokenizer_name=ENCODER_NAME,
        max_length=ENC_MAX_LENGTH,        # 인코더 길이
        stride=STRIDE,
        limit=None,
        decoder_tokenizer=dec_tok,
        dec_max_length=DEC_MAX_LENGTH,    # 디코더 길이
    )
    kwargs = dict(
        dataset=dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available(),
    )
    if num_workers > 0:
        kwargs.update(prefetch_factor=4, persistent_workers=True)
    return DataLoader(**kwargs)


# -----------------------------
# 모델/옵티마이저/스케줄러
# -----------------------------
def build_model(dec_tok: T5TokenizerFast, device: torch.device) -> CorrectionModel:
    model = CorrectionModel(
        encoder_name=ENCODER_NAME,
        num_decoder_layers=6,
        decoder_tokenizer=dec_tok,
    )
    model.to(device)
    return model


def build_optimizer(model: torch.nn.Module, lr: float, weight_decay: float) -> torch.optim.Optimizer:
    # Decay/No-decay 분리
    decay, no_decay = [], []
    for n, p in model.named_parameters():
        if not p.requires_grad: 
            continue
        if any(nd in n for nd in ["bias", "LayerNorm.weight", "layer_norm.weight", "layernorm.weight"]):
            no_decay.append(p)
        else:
            decay.append(p)
    return torch.optim.AdamW(
        [{"params": decay, "weight_decay": weight_decay},
         {"params": no_decay, "weight_decay": 0.0}],
        lr=lr, betas=(0.9, 0.98), eps=1e-8
    )


def build_scheduler(optimizer, total_steps: int, warmup_steps: int, use_cosine: bool=True):
    if use_cosine:
        return get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)
    else:
        return get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)


# -----------------------------
# 손실 함수
# -----------------------------
class LabelSmoothingCE(nn.Module):
    def __init__(self, smoothing: float, ignore_index: int = -100, vocab_size: int = None):
        super().__init__()
        self.smoothing = smoothing
        self.ignore_index = ignore_index
        self.vocab_size = vocab_size

    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        # logits: (B, T, V), targets: (B, T)
        B, T, V = logits.shape
        assert targets.shape == (B, T)
        # 마스크
        pad_mask = targets.eq(self.ignore_index)  # (B, T)
        n_valid = (~pad_mask).sum().clamp_min(1)

        if self.smoothing > 0.0:
            # log-prob
            log_probs = torch.log_softmax(logits, dim=-1)  # (B,T,V)
            # NLL part
            nll_loss = -log_probs.gather(dim=-1, index=targets.masked_fill(pad_mask, 0).unsqueeze(-1)).squeeze(-1)
            nll_loss = nll_loss.masked_fill(pad_mask, 0.0).sum() / n_valid

            # uniform part
            if self.vocab_size is None:
                V = logits.size(-1)
            else:
                V = self.vocab_size
            smooth_loss = -log_probs.mean(dim=-1)  # (B,T)
            smooth_loss = smooth_loss.masked_fill(pad_mask, 0.0).sum() / n_valid

            loss = (1 - self.smoothing) * nll_loss + self.smoothing * smooth_loss
            return loss
        else:
            # 표준 CE
            loss_fct = nn.CrossEntropyLoss(ignore_index=self.ignore_index)
            return loss_fct(logits.view(B*T, V), targets.view(B*T))


def compute_loss(logits: torch.Tensor, labels: torch.Tensor, smoothing: float, vocab_size: int) -> torch.Tensor:
    return LabelSmoothingCE(smoothing=smoothing, ignore_index=-100, vocab_size=vocab_size)(logits, labels)


# -----------------------------
# 학습 스텝/에폭
# -----------------------------
def train_step(
    model: CorrectionModel,
    batch: Dict[str, torch.Tensor],
    optimizer: torch.optim.Optimizer,
    scaler: torch.cuda.amp.GradScaler,
    scheduler,
    device: torch.device,
    smoothing: float,
    grad_clip_norm: float,
    accum_steps: int,
    step_idx: int,
) -> Tuple[float, bool]:
    model.train()
    # AMP
    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available(), dtype=torch.float16):
        logits = model(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
            decoder_input_ids=batch["decoder_input_ids"],
            decoder_attention_mask=batch["decoder_attention_mask"],
        )
        loss = compute_loss(logits, batch["labels"], smoothing, vocab_size=model.dec_tok.vocab_size)
        loss = loss / accum_steps

    scaler.scale(loss).backward()

    # optimizer step (accumulation)
    did_step = False
    if (step_idx + 1) % accum_steps == 0:
        if grad_clip_norm is not None and grad_clip_norm > 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)

        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad(set_to_none=True)
        if scheduler is not None:
            scheduler.step()
        did_step = True

    return loss.item() * accum_steps, did_step


def train_one_epoch(
    model: CorrectionModel,
    loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    scheduler,
    device: torch.device,
    smoothing: float,
    grad_clip_norm: float,
    accum_steps: int,
    sanity_batches: int,
) -> None:
    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())
    running = 0.0
    count = 0
    stepped = 0

    for step_idx, batch in enumerate(loader):
        # 종료 조건: 미니 검증만
        if step_idx >= sanity_batches:
            break

        batch = {k: v.to(device) for k, v in batch.items()}
        loss_val, did_step = train_step(
            model, batch, optimizer, scaler, scheduler, device,
            smoothing, grad_clip_norm, accum_steps, step_idx
        )
        running += loss_val
        count += 1
        if did_step:
            stepped += 1

        if (step_idx + 1) % 10 == 0:
            avg = running / count
            lr = scheduler.get_last_lr()[0] if scheduler is not None else optimizer.param_groups[0]["lr"]
            print(f"[step {step_idx+1}] loss(avg): {avg:.4f} | lr: {lr:.2e} | opt-steps: {stepped}")

    print(f"[SANITY DONE] steps: {count}, optimizer-steps: {stepped}, loss(avg): {running/max(1,count):.4f}")


# -----------------------------
# Main
# -----------------------------
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[Info] Using device: {device}")

    # 토크나이저/데이터/모델
    dec_tok = build_decoder_tokenizer(DECODER_SPM_PATH, legacy=False)  # 필요시 True
    loader = build_loader(dec_tok, BATCH_SIZE, NUM_WORKERS)
    model = build_model(dec_tok, device)

    # 옵티마이저/스케줄러
    optimizer = build_optimizer(model, LR, WEIGHT_DECAY)
    scheduler = build_scheduler(optimizer, TOTAL_TRAIN_STEPS, NUM_WARMUP_STEPS, use_cosine=USE_COSINE)

    # 미니 학습 실행
    train_one_epoch(
        model=model,
        loader=loader,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        smoothing=LABEL_SMOOTHING,
        grad_clip_norm=GRAD_CLIP_NORM,
        accum_steps=ACCUM_STEPS,
        sanity_batches=SANITY_BATCHES,
    )


if __name__ == "__main__":
    main()

# train_full_split.py
# 8:1:1 분할 파일 사용 + tqdm + 에폭별 val(loss/acc/CER) + EarlyStopping
# 언프리즈: 워밍업 이후 + 최소 스텝 + 에폭 1 이후에만 점진적(뒤에서부터 k층)

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import math
from dataclasses import dataclass
from typing import Dict, Optional

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import T5TokenizerFast, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup
from tqdm.auto import tqdm

from model3 import CorrectionModel
from data_txt_stream import GrammarCorrectionDataset

# ========= 경로 =========
ENCODER_NAME    = "beomi/kcbert-base"
DECODER_SPM     = "../spm_out/tokenizer_32k.model"
KO_TRAIN        = "../ko.train.txt"
COR_TRAIN       = "../correct.train.txt"
KO_VAL          = "../ko.val.txt"
COR_VAL         = "../correct.val.txt"
KO_TEST         = "../ko.test.txt"
COR_TEST        = "../correct.test.txt"
TOTAL_LINES     = 2_228_281  # 정보용

# ========= 데이터/로더 =========
ENC_MAX_LEN     = 300
DEC_MAX_LEN     = 256
BATCH_SIZE      = 8
NUM_WORKERS     = 0                 # IterableDataset은 0이 안전(중복 방지)
ACCUM_STEPS     = 1
PIN_MEMORY      = torch.cuda.is_available()

# ========= 학습 =========
EPOCHS          = 30
LR_DEC          = 3e-4
LR_ENC          = 1e-5
WEIGHT_DECAY    = 0.01
WARMUP_STEPS    = 10_000
USE_COSINE      = True
LABEL_SMOOTHING = 0.1
GRAD_CLIP_NORM  = 1.0

# ========= 언프리즈/플래토 =========
UNFREEZE_START_LAYERS = 0
UNFREEZE_STEP_LAYERS  = 2
UNFREEZE_MAX_LAYERS   = 12
PATIENCE_STEPS_TRAIN  = 20_000   # 5k → 20k (초반 노이즈에 덜 민감)
MIN_IMPROVE_RATIO     = 0.0002   # 0.1% → 0.02% (작은 개선도 인정)
EMA_BETA              = 0.98
MIN_STEPS_FOR_UNFREEZE = 30_000  # 최소 3만 스텝은 디코더만

# ========= Early Stopping =========
ES_MONITOR     = "val_loss"   # or "val_acc"
ES_MIN_DELTA   = 0.0
ES_PATIENCE_EP = 3

# ========= 저장/로그 =========
CKPT_DIR   = "./checkpoints"
MILESTONE_EVERY_EPOCHS = 3
SAVE_EVERY_STEPS = 20_000
os.makedirs(CKPT_DIR, exist_ok=True)

# ========= 평가 옵션 =========
EVAL_CER_MAX_SAMPLES = 5_000
IGNORE_INDEX = -100

# ========= CER 피크 주기/규모 =========
PEEK_EVERY_STEPS = 2000
PEEK_MAX_BATCHES = 4
PEEK_MAX_SAMPLES = 400


# --------------------------
# 토크나이저
# --------------------------
def ensure_special_tokens(tok: T5TokenizerFast) -> T5TokenizerFast:
    changed = False
    if tok.pad_token is None:
        tok.add_special_tokens({"pad_token": "<pad>"}); changed = True
    if getattr(tok, "bos_token", None) is None:
        tok.add_special_tokens({"bos_token": "<s>"}); changed = True
    if tok.eos_token is None:
        tok.add_special_tokens({"eos_token": "</s>"}); changed = True
    if changed:
        print("[Info] Added missing special tokens to decoder tokenizer.")
    return tok

def build_dec_tok(spm_path: str, legacy: bool=False) -> T5TokenizerFast:
    if not os.path.exists(spm_path):
        raise FileNotFoundError(spm_path)
    tok = T5TokenizerFast(vocab_file=spm_path, extra_ids=0, legacy=legacy)
    return ensure_special_tokens(tok)


# --------------------------
# 모델/옵티마이저/스케줄러/손실
# --------------------------
def build_model(dec_tok: T5TokenizerFast, device) -> CorrectionModel:
    m = CorrectionModel(
        encoder_name=ENCODER_NAME,
        num_decoder_layers=6,
        decoder_tokenizer=dec_tok,
    )
    m.to(device)
    return m

def split_params_for_groups(model: CorrectionModel):
    enc_decay, enc_nodecay, dec_decay, dec_nodecay = [], [], [], []
    for n, p in model.encoder.named_parameters():
        (enc_nodecay if any(x in n for x in ["bias","LayerNorm.weight","layer_norm.weight","layernorm.weight"]) else enc_decay).append(p)
    for n, p in model.decoder.named_parameters():
        (dec_nodecay if any(x in n for x in ["bias","LayerNorm.weight","layer_norm.weight","layernorm.weight"]) else dec_decay).append(p)
    for n, p in model.output_projection.named_parameters():
        (dec_nodecay if any(x in n for x in ["bias","LayerNorm.weight","layer_norm.weight","layernorm.weight"]) else dec_decay).append(p)
    return enc_decay, enc_nodecay, dec_decay, dec_nodecay

def build_optimizer(model: CorrectionModel):
    enc_d, enc_n, dec_d, dec_n = split_params_for_groups(model)
    return torch.optim.AdamW(
        [
            {"params": enc_d, "weight_decay": WEIGHT_DECAY, "lr": LR_ENC},
            {"params": enc_n, "weight_decay": 0.0,          "lr": LR_ENC},
            {"params": dec_d, "weight_decay": WEIGHT_DECAY, "lr": LR_DEC},
            {"params": dec_n, "weight_decay": 0.0,          "lr": LR_DEC},
        ],
        betas=(0.9, 0.98), eps=1e-8
    )

def build_scheduler(optimizer, total_steps: int):
    if USE_COSINE:
        return get_cosine_schedule_with_warmup(optimizer, WARMUP_STEPS, total_steps)
    else:
        return get_linear_schedule_with_warmup(optimizer, WARMUP_STEPS, total_steps)

class LabelSmoothingCE(nn.Module):
    def __init__(self, smoothing: float, ignore_index: int = -100):
        super().__init__()
        self.smoothing = smoothing
        self.ignore_index = ignore_index
    def forward(self, logits, targets):
        B, T, V = logits.shape
        pad_mask = targets.eq(self.ignore_index)
        n_valid = (~pad_mask).sum().clamp_min(1)
        if self.smoothing > 0.0:
            log_probs = torch.log_softmax(logits, dim=-1)
            nll = -log_probs.gather(dim=-1, index=targets.masked_fill(pad_mask, 0).unsqueeze(-1)).squeeze(-1)
            nll = nll.masked_fill(pad_mask, 0.0).sum() / n_valid
            smooth = -log_probs.mean(dim=-1)
            smooth = smooth.masked_fill(pad_mask, 0.0).sum() / n_valid
            return (1 - self.smoothing) * nll + self.smoothing * smooth
        else:
            return nn.CrossEntropyLoss(ignore_index=self.ignore_index)(logits.view(B*T, V), targets.view(B*T))

criterion = LabelSmoothingCE(LABEL_SMOOTHING, ignore_index=IGNORE_INDEX)


# --------------------------
# 언프리즈/학습 상태
# --------------------------
def set_encoder_trainable_layers(model: CorrectionModel, k_last_layers: int):
    for p in model.encoder.parameters():
        p.requires_grad = False
    if k_last_layers > 0:
        layers = list(model.encoder.encoder.layer)
        start = max(0, len(layers) - k_last_layers)
        for i, layer in enumerate(layers):
            for p in layer.parameters():
                p.requires_grad = (i >= start)
    for p in model.encoder.embeddings.parameters():
        p.requires_grad = False
    if hasattr(model.encoder, "pooler") and model.encoder.pooler is not None:
        for p in model.encoder.pooler.parameters():
            p.requires_grad = False

@dataclass
class TrainState:
    global_step: int = 0
    ema_loss: Optional[float] = None
    best_train_ema: float = float("inf")
    steps_since_improve: int = 0
    unfrozen_layers: int = UNFREEZE_START_LAYERS

@dataclass
class ESState:
    best_metric: float = float("inf") if ES_MONITOR=="val_loss" else -float("inf")
    best_epoch: int = -1
    wait: int = 0

def is_better(curr, best):
    return (curr < best) if ES_MONITOR=="val_loss" else (curr > best)


# --------------------------
# CER / 평가
# --------------------------
def cer_one(ref: str, hyp: str) -> float:
    R, H = ref, hyp
    n, m = len(R), len(H)
    if n == 0: return float(m > 0)
    dp = [[0]*(m+1) for _ in range(n+1)]
    for i in range(n+1): dp[i][0] = i
    for j in range(m+1): dp[0][j] = j
    for i in range(1, n+1):
        ri = R[i-1]
        for j in range(1, m+1):
            hj = H[j-1]
            dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+(ri!=hj))
    return dp[n][m] / max(1, n)

@torch.no_grad()
def evaluate(model: CorrectionModel, loader: DataLoader, device) -> Dict[str, float]:
    model.eval()
    total_loss, total_tok, total_cor, total_cer, n_cer = 0.0, 0, 0, 0.0, 0
    dec_tok = model.dec_tok
    pad_id = dec_tok.pad_token_id

    for batch in tqdm(loader, desc="Validate/Test", leave=False):
        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}
        logits = model(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
            decoder_input_ids=batch["decoder_input_ids"],
            decoder_attention_mask=batch["decoder_attention_mask"],
        )
        loss = criterion(logits, batch["labels"]).item()
        total_loss += loss

        preds = logits.argmax(-1)
        golds = batch["labels"]
        mask  = golds.ne(IGNORE_INDEX)
        total_tok += mask.sum().item()
        total_cor += (preds.eq(golds) & mask).sum().item()

        if EVAL_CER_MAX_SAMPLES == 0 or n_cer < EVAL_CER_MAX_SAMPLES:
            B = preds.size(0)
            for b in range(B):
                if EVAL_CER_MAX_SAMPLES and n_cer >= EVAL_CER_MAX_SAMPLES: break
                hyp  = dec_tok.decode(preds[b].tolist(), skip_special_tokens=True)
                gold = [pad_id if t == IGNORE_INDEX else t for t in golds[b].tolist()]
                ref  = dec_tok.decode(gold, skip_special_tokens=True)
                total_cer += cer_one(ref, hyp)
                n_cer += 1

    avg_loss = total_loss / max(1, len(loader))
    acc = total_cor / max(1, total_tok) if total_tok > 0 else 0.0
    avg_cer = total_cer / max(1, n_cer) if n_cer > 0 else 0.0
    return {"loss": avg_loss, "acc": acc, "cer": avg_cer}

@torch.no_additional_grad = torch.no_grad  # (just to avoid linter warning if any)
@torch.no_grad()
def evaluate_peek(model: CorrectionModel, val_loader: DataLoader, device, max_batches=4, max_samples=400):
    """학습 중 가벼운 CER 추정: val 로더에서 앞쪽 몇 배치만 훑음."""
    model.eval()
    dec_tok = model.dec_tok
    pad_id = dec_tok.pad_token_id
    total_cer, n = 0.0, 0

    it = iter(val_loader)
    for _ in range(max_batches):
        try:
            batch = next(it)
        except StopIteration:
            break
        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}
        logits = model(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
            decoder_input_ids=batch["decoder_input_ids"],
            decoder_attention_mask=batch["decoder_attention_mask"],
        )
        preds = logits.argmax(-1)
        golds = batch["labels"]
        B = preds.size(0)
        for b in range(B):
            if n >= max_samples: break
            hyp  = dec_tok.decode(preds[b].tolist(), skip_special_tokens=True)
            gold = [pad_id if t == IGNORE_INDEX else t for t in golds[b].tolist()]
            ref  = dec_tok.decode(gold, skip_special_tokens=True)
            total_cer += cer_one(ref, hyp); n += 1
        if n >= max_samples: break

    return (total_cer / n) if n > 0 else None


# --------------------------
# 저장
# --------------------------
def save_ckpt(model: CorrectionModel, tag: str, step: int):
    path = os.path.join(CKPT_DIR, f"step_{step}_{tag}.pt")
    torch.save(model.state_dict(), path)
    print(f"[Save] {path}")


# --------------------------
# 메인 학습
# --------------------------
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[Info] device: {device}")

    dec_tok = build_dec_tok(DECODER_SPM, legacy=False)
    print(f"[Info] Decoder vocab: {dec_tok.vocab_size}, PAD/BOS/EOS={dec_tok.pad_token_id}/{getattr(dec_tok,'bos_token_id',None)}/{dec_tok.eos_token_id}")

    model = build_model(dec_tok, device)
    set_encoder_trainable_layers(model, UNFREEZE_START_LAYERS)
    optimizer = build_optimizer(model)

    # train steps/epoch (대략): train 라인 수 / batch
    train_lines = 1_782_624
    steps_tr_ep = math.ceil(train_lines / (BATCH_SIZE * max(1, ACCUM_STEPS)))
    scheduler = build_scheduler(optimizer, steps_tr_ep * EPOCHS)
    scaler = torch.amp.GradScaler(device="cuda", enabled=torch.cuda.is_available())

    # 로더
    def build_loader(ko, cor, limit=None):
        ds = GrammarCorrectionDataset(
            ko_file=ko, correct_file=cor,
            tokenizer_name=ENCODER_NAME, max_length=ENC_MAX_LEN, stride=0,
            limit=limit, decoder_tokenizer=dec_tok,
            dec_max_length=DEC_MAX_LEN, downsample_prob=1.0,
        )
        return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)

    train_loader = build_loader(KO_TRAIN, COR_TRAIN, limit=None)
    val_loader   = build_loader(KO_VAL,   COR_VAL,   limit=None)
    test_loader  = build_loader(KO_TEST,  COR_TEST,  limit=None)

    state, es = TrainState(), ESState()

    for epoch in range(EPOCHS):
        model.train()
        pbar = tqdm(total=steps_tr_ep, desc=f"Epoch {epoch+1}/{EPOCHS} [train]", leave=True)
        accum, running_loss, running_steps = 0, 0.0, 0

        train_iter = iter(train_loader)
        while state.global_step // max(1, ACCUM_STEPS) < (epoch+1) * steps_tr_ep:
            try:
                batch = next(train_iter)
            except StopIteration:
                train_iter = iter(train_loader); batch = next(train_iter)

            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}
            with torch.amp.autocast(device_type="cuda", dtype=torch.float16, enabled=torch.cuda.is_available()):
                logits = model(
                    input_ids=batch["input_ids"],
                    attention_mask=batch["attention_mask"],
                    decoder_input_ids=batch["decoder_input_ids"],
                    decoder_attention_mask=batch["decoder_attention_mask"],
                )
                loss = criterion(logits, batch["labels"]) / ACCUM_STEPS

            scaler.scale(loss).backward(); accum += 1

            if accum >= ACCUM_STEPS:
                if GRAD_CLIP_NORM and GRAD_CLIP_NORM > 0:
                    scaler.unscale_(optimizer)
                    nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)
                scaler.step(optimizer); scaler.update()
                optimizer.zero_grad(set_to_none=True)
                scheduler.step()

                state.global_step += 1
                accum = 0

                # 로그 누적 (optimizer step 기준)
                l = loss.item() * ACCUM_STEPS
                running_loss += l; running_steps += 1
                state.ema_loss = l if state.ema_loss is None else EMA_BETA*state.ema_loss + (1-EMA_BETA)*l

                # 가벼운 CER 피크 (PEEK_EVERY_STEPS마다 소량 배치만)
                if (state.global_step % PEEK_EVERY_STEPS) == 0:
                    peek_cer = evaluate_peek(model, val_loader, device, max_batches=PEEK_MAX_BATCHES, max_samples=PEEK_MAX_SAMPLES)
                else:
                    peek_cer = None

                # tqdm
                pf = {"loss": f"{l:.2f}", "ema": f"{state.ema_loss:.2f}",
                      "lr": f"{optimizer.param_groups[2]['lr']:.2e}", "unfrz": state.unfrozen_layers}
                if peek_cer is not None:
                    pf["cer(peek)"] = f"{peek_cer:.3f}"
                pbar.update(1); pbar.set_postfix(pf)

                # (옵션) 주기 저장
                if SAVE_EVERY_STEPS and state.global_step % SAVE_EVERY_STEPS == 0:
                    save_ckpt(model, "autosave", state.global_step)

                # ===== plateau → 점진 언프리즈 (워밍업 끝 + 최소 스텝 + 에폭 1 이후) =====
                improved = (state.ema_loss + MIN_IMPROVE_RATIO * state.best_train_ema) < state.best_train_ema
                if improved:
                    state.best_train_ema = state.ema_loss
                    state.steps_since_improve = 0
                else:
                    state.steps_since_improve += 1
                    if (
                        state.steps_since_improve >= PATIENCE_STEPS_TRAIN
                        and state.unfrozen_layers < UNFREEZE_MAX_LAYERS
                        and state.global_step >= WARMUP_STEPS
                        and state.global_step >= MIN_STEPS_FOR_UNFREEZE
                        and epoch >= 1
                    ):
                        new_layers = min(UNFREEZE_MAX_LAYERS, state.unfrozen_layers + UNFREEZE_STEP_LAYERS)
                        print(f"[Unfreeze] plateau → encoder last {UNFREEZE_STEP_LAYERS} layers: "
                              f"{state.unfrozen_layers} → {new_layers} (step={state.global_step}, epoch={epoch+1})")
                        state.unfrozen_layers = new_layers
                        set_encoder_trainable_layers(model, state.unfrozen_layers)
                        state.steps_since_improve = 0

        pbar.close()

        # Epoch 평균 로그
        train_avg_loss = running_loss / max(1, running_steps)
        print(f"[train] epoch {epoch+1} | avg_loss {train_avg_loss:.4f} | ema {state.ema_loss:.4f} | "
              f"lr {optimizer.param_groups[2]['lr']:.2e} | unfrz {state.unfrozen_layers}")

        # Validation
        val_metrics = evaluate(model, val_loader, device)
        print(f"[val] epoch {epoch+1} | loss {val_metrics['loss']:.4f} | acc {val_metrics['acc']*100:.2f}% | cer {val_metrics['cer']:.4f}")

        # 저장
        save_ckpt(model, f"epoch{epoch+1}", state.global_step)
        if (epoch + 1) % MILESTONE_EVERY_EPOCHS == 0:
            save_ckpt(model, f"epoch{epoch+1}_milestone", state.global_step)

        # Early Stop
        monitor_value = val_metrics["loss"] if ES_MONITOR == "val_loss" else val_metrics["acc"]
        better = (monitor_value < es.best_metric - ES_MIN_DELTA) if ES_MONITOR=="val_loss" else (monitor_value > es.best_metric + ES_MIN_DELTA)
        if better:
            es.best_metric = monitor_value; es.best_epoch = epoch + 1; es.wait = 0
            save_ckpt(model, f"best_{ES_MONITOR}", state.global_step)
        else:
            es.wait += 1
            if es.wait >= ES_PATIENCE_EP:
                print(f"[EarlyStop] No improvement in {ES_MONITOR} for {ES_PATIENCE_EP} epochs (best at epoch {es.best_epoch}).")
                break

    # Test
    test_metrics = evaluate(model, test_loader, device)
    print(f"[test] loss {test_metrics['loss']:.4f} | acc {test_metrics['acc']*100:.2f}% | cer {test_metrics['cer']:.4f}")
    print("[Done]")

if __name__ == "__main__":
    main()

root@986d73f0cf4d:/workspace/sentence/src# python3 check.py
Using device: cuda
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
{'input_ids': tensor([[    2, 27055,    17,  ...,     0,     0,     0],
        [    2,  2170,  8050,  ...,     0,     0,     0],
        [    2,  8881, 20557,  ...,     0,     0,     0],
        ...,
        [    2, 17275, 18567,  ...,     0,     0,     0],
        [    2, 18534,  8042,  ...,     0,     0,     0],
        [    2,  9584, 24697,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'labels': tensor([[    2, 27055,    17,  ...,  -100,  -100,  -100],
        [    2,  2170,  8050,  ...,  -100,  -100,  -100],
        [    2,  2232,  8881,  ...,  -100,  -100,  -100],
        ...,
        [    2, 17275, 18567,  ...,  -100,  -100,  -100],
        [    2, 18534,  8042,  ...,  -100,  -100,  -100],
        [    2,  9584, 24697,  ...,  -100,  -100,  -100]], device='cuda:0')}
Traceback (most recent call last):
  File "/workspace/sentence/src/check.py", line 53, in <module>
    assert max_label < model.dec_tok.vocab_size, f"Invalid label found: {max_label} is greater than vocab size {model.dec_tok.vocab_size}"
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Invalid label found: 29827 is greater than vocab size 24000

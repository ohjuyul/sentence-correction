from transformers import PretrainedConfig

class BartConfig(PretrainedConfig):
    model_type = "bart"

    def __init__(
        self,
        vocab_size=30000,
        d_model=768,
        decoder_layers=6,
        decoder_attention_heads=12,
        **kwargs,
    ):
        super().__init__(vocab_size=vocab_size, **kwargs)
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.decoder_layers = decoder_layers
        self.decoder_attention_heads = decoder_attention_heads
        self.pad_token_id = 0
        self.dropout = 0.1
        self.attention_dropout = 0.0
        self.activation_function = "gelu"
        self.activation_dropout = 0.0
        self.scale_embedding = False
        self.use_cache = True
        self.max_position_embeddings = 512
        self.decoder_ffn_dim = 3072

import re
import bz2
import argparse
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Iterable, List, Tuple, Generator, Optional

import sentencepiece as spm


# -------------------------------
# 0) 파일 열기 (xml / xml.bz2 지원)
# -------------------------------
def open_xml_stream(path: str):
    p = Path(path)
    if p.suffix == ".bz2":
        return bz2.open(p, "rb")
    return open(p, "rb")


# -------------------------------
# 1) 위키 XML에서 본문 텍스트 스트림 추출
#    (namespace=0 문서만)
# -------------------------------
def iter_wiki_texts(xml_path: str) -> Generator[str, None, None]:
    """
    메모리 안전을 위해 iterparse 사용, page 단위로 element clear.
    """
    with open_xml_stream(xml_path) as f:
        # 네임스페이스 유연 처리: .endswith로 태그 판별
        context = ET.iterparse(f, events=("end",))
        page_title = None
        page_ns = None
        page_text = None

        for event, elem in context:
            tag = elem.tag
            if tag.endswith("title"):
                page_title = elem.text or ""
            elif tag.endswith("ns"):
                page_ns = elem.text or "0"
            elif tag.endswith("text"):
                page_text = elem.text or ""
            elif tag.endswith("page"):
                if page_ns == "0" and page_text:
                    yield page_text
                # 메모리 해제
                elem.clear()
                page_title = page_ns = page_text = None


# -------------------------------
# 2) 위키 마크업 간이 정리
#    (외부 라이브러리 없이 UNK 체크용 최소 정리)
# -------------------------------
_ref_re = re.compile(r"<ref[^>]*?>.*?</ref>", re.DOTALL)
_ref_self_re = re.compile(r"<ref[^>]*/>", re.DOTALL)
_comment_re = re.compile(r"<!--.*?-->", re.DOTALL)
_html_re = re.compile(r"<[^>]+>", re.DOTALL)
_tpl_re = re.compile(r"\{\{[^{}]*\}\}")  # 중첩 약함(간이)
_link_file_re = re.compile(r"\[\[(파일|이미지):.*?\]\]", re.IGNORECASE)
_category_re = re.compile(r"\[\[(분류|category):.*?\]\]", re.IGNORECASE)
_extlink_re = re.compile(r"\[https?://[^\s\]]+(?:\s+[^\]]+)?\]")
_link_re = re.compile(r"\[\[([^|\]]+)\|([^\]]+)\]\]")  # [[a|b]] -> b
_link_simple_re = re.compile(r"\[\[([^\]]+)\]\]")      # [[a]]   -> a
_heading_re = re.compile(r"^\s*=+\s*(.*?)\s*=+\s*$", re.MULTILINE)

def clean_wikitext(w: str) -> str:
    # 주석/참고문헌/HTML 태그/카테고리/파일/외부링크 제거
    w = _comment_re.sub(" ", w)
    w = _ref_re.sub(" ", w)
    w = _ref_self_re.sub(" ", w)
    w = _link_file_re.sub(" ", w)
    w = _category_re.sub(" ", w)
    w = _extlink_re.sub(" ", w)
    # 템플릿 간이 제거 (중첩 다 못 잡아도 UNK 체크엔 충분)
    for _ in range(3):  # 몇 번 반복 제거
        new_w = _tpl_re.sub(" ", w)
        if new_w == w:
            break
        w = new_w
    # 내부 링크 처리
    w = _link_re.sub(r"\2", w)
    w = _link_simple_re.sub(r"\1", w)
    # 헤딩 정리
    w = _heading_re.sub(r"\1", w)
    # 잔여 HTML 태그 제거
    w = _html_re.sub(" ", w)
    # 남는 중괄호/대괄호 등 최소 정리
    w = w.replace("{", " ").replace("}", " ").replace("[", " ").replace("]", " ")
    # 공백 정리
    w = re.sub(r"[ \t\r\f\v]+", " ", w)
    # 줄바꿈 통일
    w = re.sub(r"\n{2,}", "\n", w)
    return w.strip()


# -------------------------------
# 3) 문장 분할 (간단 한국어 세그먼트)
#    - 외부 의존성 없이 마침표류 기준 분할
# -------------------------------
_sentence_split_re = re.compile(
    r"([\.!\?…]+[\"'”’)]*\s+)"  # 문장 끝부호 + 뒤따르는 공백/따옴표
)

def split_sentences_kor(text: str) -> List[str]:
    parts = _sentence_split_re.split(text)
    if not parts:
        return []
    sents = []
    buf = ""
    for i in range(0, len(parts), 2):
        seg = parts[i]
        buf += seg
        if i + 1 < len(parts):
            end = parts[i + 1]
            buf += end
            s = buf.strip()
            if s:
                sents.append(s)
            buf = ""
    if buf.strip():
        sents.append(buf.strip())
    # 너무 짧거나 긴 문장은 제외 (잡음 제거)
    out = [s for s in sents if 5 <= len(s) <= 400]
    return out


# -------------------------------
# 4) 문장 샘플링 및 파일 저장
# -------------------------------
def extract_sentence_samples(xml_path: str, sample: int = 50000) -> List[str]:
    """
    앞에서부터 모아서 sample개를 채우면 중단 (빠르고 간단한 방식).
    충분히 큰 덤프에서는 통계적으로 큰 차이 없음.
    """
    collected: List[str] = []
    for raw in iter_wiki_texts(xml_path):
        clean = clean_wikitext(raw)
        sents = split_sentences_kor(clean)
        for s in sents:
            collected.append(s)
            if len(collected) >= sample:
                return collected
    return collected


def save_sentences(path: str, sents: List[str]):
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    with p.open("w", encoding="utf-8") as f:
        for s in sents:
            f.write(s.strip() + "\n")


# -------------------------------
# 5) UNK 비율 계산
# -------------------------------
def check_unk_ratio_on_lines(sp_model: str, lines: List[str]) -> Tuple[float, float]:
    sp = spm.SentencePieceProcessor(model_file=sp_model)
    unk_id = sp.unk_id()
    total_tok = 0
    unk_tok = 0
    for s in lines:
        ids = sp.encode(s, out_type=int)
        total_tok += len(ids)
        unk_tok += sum(1 for i in ids if i == unk_id)
    unk_ratio = (unk_tok / total_tok) if total_tok else 0.0
    avg_len = (total_tok / max(1, len(lines))) if lines else 0.0
    return unk_ratio, avg_len


# -------------------------------
# 6) 메인
# -------------------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--xml", required=True, help="kowiki 덤프 경로 (.xml 또는 .xml.bz2)")
    ap.add_argument("--spm", required=True, help="SentencePiece 모델 경로 (.model)")
    ap.add_argument("--out", default="", help="추출 문장 저장 경로(txt). 비우면 저장 안 함")
    ap.add_argument("--sample", type=int, default=50000, help="평가용 문장 샘플 수")
    args = ap.parse_args()

    # 1) 문장 샘플 추출
    sents = extract_sentence_samples(args.xml, sample=args.sample)
    if not sents:
        print("문장 추출 실패 또는 문장 수 0")
        return
    print(f"[extract] sampled sentences: {len(sents)}")

    # 2) (선택) 저장
    if args.out:
        save_sentences(args.out, sents)
        print(f"[save] wrote: {args.out}")

    # 3) UNK 비율 계산
    unk_ratio, avg_len = check_unk_ratio_on_lines(args.spm, sents)
    print(f"[UNK] ratio: {unk_ratio*100:.3f}% | avg tokens/line: {avg_len:.2f}")


if __name__ == "__main__":
    main()

# train_full_split.py
# 2.23M 스트리밍 + 8:1:1 분할 + tqdm + 에폭별 val(loss/acc/CER) + EarlyStopping(patience=3)
# 디코더 우선 학습 → plateau 시 인코더 점진적 언프리즈(워밍업 이후, 뒤에서부터 k층씩)

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import math
from dataclasses import dataclass
from typing import Dict, Optional, Iterator

import torch
import torch.nn as nn
from torch.utils.data import IterableDataset, DataLoader
from transformers import T5TokenizerFast, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup
from tqdm.auto import tqdm

from model3 import CorrectionModel
from data_txt_stream import GrammarCorrectionDataset
from transformers import BertTokenizer, PreTrainedTokenizerBase

# ========= 사용자 경로 =========
ENCODER_NAME    = "beomi/kcbert-base"
DECODER_SPM     = "../spm_out/tokenizer_32k.model"
KO_TRAIN        = "../ko.train.txt"
COR_TRAIN       = "../correct.train.txt"
KO_VAL          = "../ko.val.txt"
COR_VAL         = "../correct.val.txt"
KO_TEST         = "../ko.test.txt"
COR_TEST        = "../correct.test.txt"
TOTAL_LINES     = 2_228_281

# ========= 데이터/로더 =========
ENC_MAX_LEN     = 300
DEC_MAX_LEN     = 256
BATCH_SIZE      = 8
NUM_WORKERS     = 0         # ⚠️ 안전하게 0 (IterableDataset 중복 방지)
ACCUM_STEPS     = 1
PIN_MEMORY      = torch.cuda.is_available()

# ========= 학습 =========
EPOCHS          = 30
LR_DEC          = 3e-4
LR_ENC          = 1e-5
WEIGHT_DECAY    = 0.01
WARMUP_STEPS    = 10_000
USE_COSINE      = True
LABEL_SMOOTHING = 0.1
GRAD_CLIP_NORM  = 1.0

# ========= 언프리즈 =========
UNFREEZE_START_LAYERS = 0
UNFREEZE_STEP_LAYERS  = 2
UNFREEZE_MAX_LAYERS   = 12
PATIENCE_STEPS_TRAIN  = 5_000
MIN_IMPROVE_RATIO     = 0.001
EMA_BETA              = 0.98

# ========= Early Stopping =========
ES_MONITOR     = "val_loss"
ES_MIN_DELTA   = 0.0
ES_PATIENCE_EP = 3

# ========= 저장/로그 =========
CKPT_DIR   = "./checkpoints"
MILESTONE_EVERY_EPOCHS = 3
SAVE_EVERY_STEPS = 20_000
os.makedirs(CKPT_DIR, exist_ok=True)

# ========= 평가 옵션 =========
EVAL_CER_MAX_SAMPLES = 5_000
IGNORE_INDEX = -100


# --------------------------
# 토크나이저
# --------------------------
def ensure_special_tokens(tok: T5TokenizerFast) -> T5TokenizerFast:
    changed = False
    if tok.pad_token is None:
        tok.add_special_tokens({"pad_token": "<pad>"}); changed = True
    if getattr(tok, "bos_token", None) is None:
        tok.add_special_tokens({"bos_token": "<s>"}); changed = True
    if tok.eos_token is None:
        tok.add_special_tokens({"eos_token": "</s>"}); changed = True
    if changed:
        print("[Info] Added missing special tokens to decoder tokenizer.")
    return tok

def build_dec_tok(spm_path: str, legacy: bool=False) -> T5TokenizerFast:
    if not os.path.exists(spm_path):
        raise FileNotFoundError(spm_path)
    tok = T5TokenizerFast(vocab_file=spm_path, extra_ids=0, legacy=legacy)
    return ensure_special_tokens(tok)


# --------------------------
# 모델/옵티마이저/스케줄러/손실
# --------------------------
def build_model(dec_tok: T5TokenizerFast, device) -> CorrectionModel:
    m = CorrectionModel(
        encoder_name=ENCODER_NAME,
        num_decoder_layers=6,
        decoder_tokenizer=dec_tok,
    )
    m.to(device)
    return m

def split_params_for_groups(model: CorrectionModel):
    enc_decay, enc_nodecay, dec_decay, dec_nodecay = [], [], [], []
    for n, p in model.encoder.named_parameters():
        (enc_nodecay if any(x in n for x in ["bias","LayerNorm.weight","layer_norm.weight","layernorm.weight"]) else enc_decay).append(p)
    for n, p in model.decoder.named_parameters():
        (dec_nodecay if any(x in n for x in ["bias","LayerNorm.weight","layer_norm.weight","layernorm.weight"]) else dec_decay).append(p)
    for n, p in model.output_projection.named_parameters():
        (dec_nodecay if any(x in n for x in ["bias","LayerNorm.weight","layer_norm.weight","layernorm.weight"]) else dec_decay).append(p)
    return enc_decay, enc_nodecay, dec_decay, dec_nodecay

def build_optimizer(model: CorrectionModel):
    enc_d, enc_n, dec_d, dec_n = split_params_for_groups(model)
    return torch.optim.AdamW(
        [
            {"params": enc_d, "weight_decay": WEIGHT_DECAY, "lr": LR_ENC},
            {"params": enc_n, "weight_decay": 0.0,          "lr": LR_ENC},
            {"params": dec_d, "weight_decay": WEIGHT_DECAY, "lr": LR_DEC},
            {"params": dec_n, "weight_decay": 0.0,          "lr": LR_DEC},
        ],
        betas=(0.9, 0.98), eps=1e-8
    )

def build_scheduler(optimizer, total_steps: int):
    if USE_COSINE:
        return get_cosine_schedule_with_warmup(optimizer, WARMUP_STEPS, total_steps)
    else:
        return get_linear_schedule_with_warmup(optimizer, WARMUP_STEPS, total_steps)

class LabelSmoothingCE(nn.Module):
    def __init__(self, smoothing: float, ignore_index: int = -100):
        super().__init__()
        self.smoothing = smoothing
        self.ignore_index = ignore_index
    def forward(self, logits, targets):
        B, T, V = logits.shape
        pad_mask = targets.eq(self.ignore_index)
        n_valid = (~pad_mask).sum().clamp_min(1)
        if self.smoothing > 0.0:
            log_probs = torch.log_softmax(logits, dim=-1)
            nll = -log_probs.gather(dim=-1, index=targets.masked_fill(pad_mask, 0).unsqueeze(-1)).squeeze(-1)
            nll = nll.masked_fill(pad_mask, 0.0).sum() / n_valid
            smooth = -log_probs.mean(dim=-1)
            smooth = smooth.masked_fill(pad_mask, 0.0).sum() / n_valid
            return (1 - self.smoothing) * nll + self.smoothing * smooth
        else:
            return nn.CrossEntropyLoss(ignore_index=self.ignore_index)(logits.view(B*T, V), targets.view(B*T))

criterion = LabelSmoothingCE(LABEL_SMOOTHING, ignore_index=IGNORE_INDEX)


# --------------------------
# 언프리즈/학습 상태
# --------------------------
def set_encoder_trainable_layers(model: CorrectionModel, k_last_layers: int):
    for p in model.encoder.parameters():
        p.requires_grad = False
    if k_last_layers > 0:
        layers = list(model.encoder.encoder.layer)
        start = max(0, len(layers) - k_last_layers)
        for i, layer in enumerate(layers):
            for p in layer.parameters():
                p.requires_grad = (i >= start)
    for p in model.encoder.embeddings.parameters():
        p.requires_grad = False
    if hasattr(model.encoder, "pooler") and model.encoder.pooler is not None:
        for p in model.encoder.pooler.parameters():
            p.requires_grad = False

@dataclass
class TrainState:
    global_step: int = 0
    ema_loss: Optional[float] = None
    best_train_ema: float = float("inf")
    steps_since_improve: int = 0
    unfrozen_layers: int = UNFREEZE_START_LAYERS

@dataclass
class ESState:
    best_metric: float = float("inf") if ES_MONITOR=="val_loss" else -float("inf")
    best_epoch: int = -1
    wait: int = 0

def is_better(curr, best):
    return (curr < best) if ES_MONITOR=="val_loss" else (curr > best)


# --------------------------
# 평가 루프 (loss/acc/CER)
# --------------------------
@torch.no_grad()
def evaluate(model: CorrectionModel, loader: DataLoader, device) -> Dict[str, float]:
    model.eval()
    total_loss, total_tok, total_cor, total_cer, n_cer = 0.0, 0, 0, 0.0, 0
    dec_tok = model.dec_tok
    pad_id, eos_id = dec_tok.pad_token_id, dec_tok.eos_token_id

    for batch in tqdm(loader, desc="Validate/Test", leave=False):
        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}
        logits = model(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
            decoder_input_ids=batch["decoder_input_ids"],
            decoder_attention_mask=batch["decoder_attention_mask"],
        )
        loss = criterion(logits, batch["labels"]).item()
        total_loss += loss

        preds = logits.argmax(-1)
        golds = batch["labels"]
        mask  = golds.ne(IGNORE_INDEX)
        total_tok += mask.sum().item()
        total_cor += (preds.eq(golds) & mask).sum().item()

        if EVAL_CER_MAX_SAMPLES == 0 or n_cer < EVAL_CER_MAX_SAMPLES:
            B = preds.size(0)
            for b in range(B):
                if EVAL_CER_MAX_SAMPLES and n_cer >= EVAL_CER_MAX_SAMPLES:
                    break
                hyp_text  = dec_tok.decode(preds[b].tolist(), skip_special_tokens=True)
                gold_text = dec_tok.decode([pad_id if t==IGNORE_INDEX else t for t in golds[b].tolist()],
                                           skip_special_tokens=True)
                total_cer += cer_one(gold_text, hyp_text)
                n_cer += 1

    avg_loss = total_loss / max(1, len(loader))
    acc = total_cor / max(1, total_tok) if total_tok > 0 else 0.0
    avg_cer = total_cer / max(1, n_cer) if n_cer > 0 else 0.0
    return {"loss": avg_loss, "acc": acc, "cer": avg_cer}

def cer_one(ref: str, hyp: str) -> float:
    R, H = ref, hyp
    n, m = len(R), len(H)
    if n == 0: return float(m > 0)
    dp = [[0]*(m+1) for _ in range(n+1)]
    for i in range(n+1): dp[i][0] = i
    for j in range(m+1): dp[0][j] = j
    for i in range(1, n+1):
        ri = R[i-1]
        for j in range(1, m+1):
            hj = H[j-1]
            dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+(ri!=hj))
    return dp[n][m] / max(1, n)


# --------------------------
# 저장 유틸
# --------------------------
def save_ckpt(model: CorrectionModel, tag: str, step: int):
    path = os.path.join(CKPT_DIR, f"step_{step}_{tag}.pt")
    torch.save(model.state_dict(), path)
    print(f"[Save] {path}")


# --------------------------
# 메인 학습 루프
# --------------------------
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[Info] device: {device}")

    dec_tok = build_dec_tok(DECODER_SPM, legacy=False)
    model = build_model(dec_tok, device)
    set_encoder_trainable_layers(model, UNFREEZE_START_LAYERS)
    optimizer = build_optimizer(model)
    steps_tr_ep = math.ceil(1_782_624 / (BATCH_SIZE*max(1,ACCUM_STEPS)))  # train 개수 기준
    scheduler = build_scheduler(optimizer, steps_tr_ep*EPOCHS)
    scaler = torch.amp.GradScaler(device="cuda", enabled=torch.cuda.is_available())

    def build_loader(ko, cor):
        ds = GrammarCorrectionDataset(ko, cor, tokenizer_name=ENCODER_NAME,
                                      max_length=ENC_MAX_LEN, stride=0,
                                      limit=None, decoder_tokenizer=dec_tok,
                                      dec_max_length=DEC_MAX_LEN, downsample_prob=1.0)
        return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False,
                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)

    train_loader = build_loader(KO_TRAIN, COR_TRAIN)
    val_loader   = build_loader(KO_VAL, COR_VAL)
    test_loader  = build_loader(KO_TEST, COR_TEST)

    state, es = TrainState(), ESState()

    for epoch in range(EPOCHS):
        model.train()
        pbar = tqdm(total=steps_tr_ep, desc=f"Epoch {epoch+1}/{EPOCHS} [train]", leave=True)
        accum, running_loss, running_steps = 0, 0.0, 0

        train_iter = iter(train_loader)
        while state.global_step // max(1,ACCUM_STEPS) < (epoch+1)*steps_tr_ep:
            try:
                batch = next(train_iter)
            except StopIteration:
                train_iter = iter(train_loader); batch = next(train_iter)

            batch = {k:v.to(device,non_blocking=True) for k,v in batch.items()}
            with torch.amp.autocast(device_type="cuda", dtype=torch.float16, enabled=torch.cuda.is_available()):
                logits = model(batch["input_ids"], batch["attention_mask"],
                               batch["decoder_input_ids"], batch["decoder_attention_mask"])
                loss = criterion(logits, batch["labels"]) / ACCUM_STEPS

            scaler.scale(loss).backward(); accum += 1
            if accum >= ACCUM_STEPS:
                if GRAD_CLIP_NORM>0:
                    scaler.unscale_(optimizer); nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)
                scaler.step(optimizer); scaler.update()
                optimizer.zero_grad(set_to_none=True); scheduler.step()
                state.global_step += 1; accum = 0

                l = loss.item()*ACCUM_STEPS; running_loss += l; running_steps += 1
                state.ema_loss = l if state.ema_loss is None else EMA_BETA*state.ema_loss+(1-EMA_BETA)*l

                pbar.update(1)
                pbar.set_postfix({"loss":f"{l:.2f}","ema":f"{state.ema_loss:.2f}",
                                  "lr":f"{optimizer.param_groups[2]['lr']:.2e}",
                                  "unfrz":state.unfrozen_layers})

                # 언프리즈 조건 (워밍업 이후)
                improved = (state.ema_loss + MIN_IMPROVE_RATIO*state.best_train_ema) < state.best_train_ema
                if improved:
                    state.best_train_ema = state.ema_loss; state.steps_since_improve=0
                else:
                    state.steps_since_improve += 1
                    if (state.steps_since_improve >= PATIENCE_STEPS_TRAIN
                        and state.unfrozen_layers < UNFREEZE_MAX_LAYERS
                        and state.global_step >= WARMUP_STEPS):   # ✅ 추가 조건
                        new_layers = min(UNFREEZE_MAX_LAYERS, state.unfrozen_layers+UNFREEZE_STEP_LAYERS)
                        print(f"[Unfreeze] plateau → encoder last {UNFREEZE_STEP_LAYERS} layers: {state.unfrozen_layers} → {new_layers}")
                        state.unfrozen_layers = new_layers
                        set_encoder_trainable_layers(model, state.unfrozen_layers)
                        state.steps_since_improve=0
        pbar.close()

        train_avg_loss = running_loss / max(1,running_steps)
        print(f"[train] epoch {epoch+1} | avg_loss {train_avg_loss:.4f} | ema {state.ema_loss:.4f}")

        val_metrics = evaluate(model, val_loader, device)
        print(f"[val] epoch {epoch+1} | loss {val_metrics['loss']:.4f} | acc {val_metrics['acc']*100:.2f}% | cer {val_metrics['cer']:.4f}")

        save_ckpt(model, f"epoch{epoch+1}", state.global_step)
        if (epoch+1)%MILESTONE_EVERY_EPOCHS==0: save_ckpt(model, f"epoch{epoch+1}_milestone", state.global_step)

        monitor_value = val_metrics["loss"] if ES_MONITOR=="val_loss" else val_metrics["acc"]
        better = is_better(monitor_value + (0 if ES_MONITOR=="val_loss" else 0),
                           es.best_metric + (ES_MIN_DELTA if ES_MONITOR=="val_loss" else -ES_MIN_DELTA))
        if better:
            es.best_metric=monitor_value; es.best_epoch=epoch+1; es.wait=0
            save_ckpt(model, f"best_{ES_MONITOR}", state.global_step)
        else:
            es.wait+=1
            if es.wait>=ES_PATIENCE_EP:
                print(f"[EarlyStop] No improvement for {ES_PATIENCE_EP} epochs (best at epoch {es.best_epoch}).")
                break

    test_metrics = evaluate(model, test_loader, device)
    print(f"[test] loss {test_metrics['loss']:.4f} | acc {test_metrics['acc']*100:.2f}% | cer {test_metrics['cer']:.4f}")
    print("[Done]")

if __name__ == "__main__":
    main()

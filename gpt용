# train_sanity.py
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from typing import Dict, Tuple
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import T5TokenizerFast, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup

from model3 import CorrectionModel
from data_txt import GrammarCorrectionDataset  # ← Streaming / IterableDataset 버전


# =============================
# 구성 (경로/모델/토크나이저)
# =============================
ENCODER_NAME = "beomi/kcbert-base"
DECODER_SPM_PATH = "../spm_out/tokenizer_32k.model"
KO_FILE = "../ko.txt"
CORRECT_FILE = "../correct.txt"

# 길이/윈도우 (스트리밍에선 STRIDE=0 권장)
ENC_MAX_LENGTH = 192
DEC_MAX_LENGTH = 256
STRIDE = 0

# 데이터/학습 파라미터
BATCH_SIZE = 4
NUM_WORKERS = 0                 # 먼저 0으로 안전 실행 → 안정화 후 2~4로 올리기
LIMIT = 50_000                  # 에폭당 사용하는 샘플 수 (None이면 파일 끝까지)
DOWNSAMPLE_PROB = 1.0           # 0.2라면 20% 확률 샘플링

LR = 3e-4
WEIGHT_DECAY = 0.01
NUM_WARMUP_STEPS = 500
TOTAL_TRAIN_STEPS = 10_000      # 스케줄러 총 스텝(필요시 조정)
USE_COSINE = True
GRAD_CLIP_NORM = 1.0
LABEL_SMOOTHING = 0.1
ACCUM_STEPS = 1                 # 2~4로 늘리면 메모리 여유↑
SANITY_BATCHES = 200            # 빠른 확인용. 더 보려면 크게.


# =============================
# 토크나이저
# =============================
def ensure_special_tokens(tok: T5TokenizerFast) -> T5TokenizerFast:
    changed = False
    if tok.pad_token is None:
        tok.add_special_tokens({"pad_token": "<pad>"}); changed = True
    if getattr(tok, "bos_token", None) is None:
        tok.add_special_tokens({"bos_token": "<s>"}); changed = True
    if tok.eos_token is None:
        tok.add_special_tokens({"eos_token": "</s>"}); changed = True
    if changed:
        print("[Info] Added missing special tokens to decoder tokenizer.")
    return tok


def build_decoder_tokenizer(spm_path: str, legacy: bool = False) -> T5TokenizerFast:
    if not os.path.exists(spm_path):
        raise FileNotFoundError(f"Decoder SPM not found: {spm_path}")
    tok = T5TokenizerFast(vocab_file=spm_path, extra_ids=0, legacy=legacy)
    return ensure_special_tokens(tok)


# =============================
# 데이터로더 (Streaming)
# =============================
def build_loader(
    dec_tok: T5TokenizerFast,
    batch_size: int,
    num_workers: int,
) -> DataLoader:
    dataset = GrammarCorrectionDataset(
        ko_file=KO_FILE,
        correct_file=CORRECT_FILE,
        tokenizer_name=ENCODER_NAME,
        max_length=ENC_MAX_LENGTH,     # 인코더 길이
        stride=STRIDE,                 # 스트리밍에선 0 권장
        limit=LIMIT,                   # 에폭당 사용할 샘플 수
        decoder_tokenizer=dec_tok,
        dec_max_length=DEC_MAX_LENGTH, # 디코더 길이
        downsample_prob=DOWNSAMPLE_PROB,
    )
    # IterableDataset → shuffle=False
    return DataLoader(
        dataset=dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available(),
    )


# =============================
# 모델/옵티마이저/스케줄러
# =============================
def build_model(dec_tok: T5TokenizerFast, device: torch.device) -> CorrectionModel:
    model = CorrectionModel(
        encoder_name=ENCODER_NAME,
        num_decoder_layers=6,
        decoder_tokenizer=dec_tok,
    )
    model.to(device)
    return model


def build_optimizer(model: torch.nn.Module, lr: float, weight_decay: float) -> torch.optim.Optimizer:
    decay, no_decay = [], []
    for n, p in model.named_parameters():
        if not p.requires_grad:
            continue
        if any(nd in n for nd in ["bias", "LayerNorm.weight", "layer_norm.weight", "layernorm.weight"]):
            no_decay.append(p)
        else:
            decay.append(p)
    return torch.optim.AdamW(
        [{"params": decay, "weight_decay": weight_decay},
         {"params": no_decay, "weight_decay": 0.0}],
        lr=lr, betas=(0.9, 0.98), eps=1e-8
    )


def build_scheduler(optimizer, total_steps: int, warmup_steps: int, use_cosine: bool=True):
    if use_cosine:
        return get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)
    else:
        return get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)


# =============================
# 손실 함수
# =============================
class LabelSmoothingCE(nn.Module):
    def __init__(self, smoothing: float, ignore_index: int = -100, vocab_size: int = None):
        super().__init__()
        self.smoothing = smoothing
        self.ignore_index = ignore_index
        self.vocab_size = vocab_size

    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        # logits: (B, T, V), targets: (B, T)
        B, T, V = logits.shape
        assert targets.shape == (B, T)
        pad_mask = targets.eq(self.ignore_index)
        n_valid = (~pad_mask).sum().clamp_min(1)

        if self.smoothing > 0.0:
            log_probs = torch.log_softmax(logits, dim=-1)
            nll_loss = -log_probs.gather(dim=-1, index=targets.masked_fill(pad_mask, 0).unsqueeze(-1)).squeeze(-1)
            nll_loss = nll_loss.masked_fill(pad_mask, 0.0).sum() / n_valid

            smooth_loss = -log_probs.mean(dim=-1)
            smooth_loss = smooth_loss.masked_fill(pad_mask, 0.0).sum() / n_valid

            return (1 - self.smoothing) * nll_loss + self.smoothing * smooth_loss
        else:
            loss_fct = nn.CrossEntropyLoss(ignore_index=self.ignore_index)
            return loss_fct(logits.view(B*T, V), targets.view(B*T))


def compute_loss(logits: torch.Tensor, labels: torch.Tensor, smoothing: float, vocab_size: int) -> torch.Tensor:
    return LabelSmoothingCE(smoothing=smoothing, ignore_index=-100, vocab_size=vocab_size)(logits, labels)


# =============================
# 학습 루틴 (AMP 최신 문법 / 스케줄러 순서 fix)
# =============================
def train_step(
    model: CorrectionModel,
    batch: Dict[str, torch.Tensor],
    optimizer: torch.optim.Optimizer,
    scaler: torch.amp.GradScaler,
    scheduler,
    device: torch.device,
    smoothing: float,
    grad_clip_norm: float,
    accum_steps: int,
    step_idx: int,
    global_opt_steps: int,
) -> Tuple[float, bool, int]:
    model.train()
    with torch.amp.autocast(device_type="cuda", dtype=torch.float16, enabled=torch.cuda.is_available()):
        logits = model(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
            decoder_input_ids=batch["decoder_input_ids"],
            decoder_attention_mask=batch["decoder_attention_mask"],
        )
        loss = compute_loss(logits, batch["labels"], smoothing, vocab_size=model.dec_tok.vocab_size)
        loss = loss / accum_steps

    scaler.scale(loss).backward()

    did_step = False
    if (step_idx + 1) % accum_steps == 0:
        if grad_clip_norm is not None and grad_clip_norm > 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)

        # 1) optimizer.step
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad(set_to_none=True)
        did_step = True

        # 2) scheduler.step  (optimizer.step 이후에 호출)
        if scheduler is not None:
            scheduler.step()

        global_opt_steps += 1

    return loss.item() * accum_steps, did_step, global_opt_steps


def train_one_epoch(
    model: CorrectionModel,
    loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    scheduler,
    device: torch.device,
    smoothing: float,
    grad_clip_norm: float,
    accum_steps: int,
    sanity_batches: int,
) -> None:
    scaler = torch.amp.GradScaler(device="cuda", enabled=torch.cuda.is_available())
    running = 0.0
    count = 0
    opt_steps = 0

    for step_idx, batch in enumerate(loader):
        if step_idx >= sanity_batches:
            break

        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}
        loss_val, did_step, opt_steps = train_step(
            model, batch, optimizer, scaler, scheduler, device,
            smoothing, grad_clip_norm, accum_steps, step_idx, opt_steps
        )
        running += loss_val
        count += 1

        if (step_idx + 1) % 10 == 0:
            avg = running / count
            # 현재 lr은 optimizer에서 직접 읽는 게 안전 (스케줄러 초반 경고 회피)
            lr = optimizer.param_groups[0]["lr"]
            print(f"[step {step_idx+1}] loss(avg): {avg:.4f} | lr: {lr:.2e} | opt-steps: {opt_steps}")

    print(f"[SANITY DONE] steps: {count}, optimizer-steps: {opt_steps}, loss(avg): {running/max(1,count):.4f}")


# =============================
# Main
# =============================
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[Info] Using device: {device}")

    # 디코더 토크나이저/데이터/모델
    dec_tok = build_decoder_tokenizer(DECODER_SPM_PATH, legacy=False)  # 필요시 legacy=True
    print(f"[Info] Decoder vocab size: {dec_tok.vocab_size}")
    print(f"[Info] PAD/BOS/EOS = {dec_tok.pad_token_id}/{getattr(dec_tok, 'bos_token_id', None)}/{dec_tok.eos_token_id}")

    loader = build_loader(dec_tok, BATCH_SIZE, NUM_WORKERS)
    model = build_model(dec_tok, device)

    # 옵티마이저/스케줄러
    optimizer = build_optimizer(model, LR, WEIGHT_DECAY)
    scheduler = build_scheduler(optimizer, TOTAL_TRAIN_STEPS, NUM_WARMUP_STEPS, use_cosine=USE_COSINE)

    # 미니 학습 실행
    train_one_epoch(
        model=model,
        loader=loader,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        smoothing=LABEL_SMOOTHING,
        grad_clip_norm=GRAD_CLIP_NORM,
        accum_steps=ACCUM_STEPS,
        sanity_batches=SANITY_BATCHES,
    )


if __name__ == "__main__":
    main()

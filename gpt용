# infer.py  — KC-BERT encoder + custom BART-style decoder inference (greedy/beam)
import os
import argparse
from typing import List, Optional

import torch
from transformers import BertTokenizer, T5TokenizerFast

from model3 import CorrectionModel


# ============== Utils ==============
def ensure_special_tokens(tok: T5TokenizerFast) -> T5TokenizerFast:
    changed = False
    if tok.pad_token is None:
        tok.add_special_tokens({"pad_token": "<pad>"}); changed = True
    if getattr(tok, "bos_token", None) is None:
        tok.add_special_tokens({"bos_token": "<s>"}); changed = True
    if tok.eos_token is None:
        tok.add_special_tokens({"eos_token": "</s>"}); changed = True
    if changed:
        print("[Info] Added missing special tokens to decoder tokenizer.")
    return tok


def load_decoder_tokenizer(spm_path: str, legacy: bool=False) -> T5TokenizerFast:
    if not os.path.exists(spm_path):
        raise FileNotFoundError(f"Decoder SPM not found: {spm_path}")
    tok = T5TokenizerFast(vocab_file=spm_path, extra_ids=0, legacy=legacy)
    return ensure_special_tokens(tok)


def build_model(encoder_name: str, dec_tok: T5TokenizerFast, ckpt_path: Optional[str], device: torch.device) -> CorrectionModel:
    model = CorrectionModel(
        encoder_name=encoder_name,
        num_decoder_layers=6,
        decoder_tokenizer=dec_tok,
    )
    if ckpt_path:
        print(f"[Info] Loading checkpoint: {ckpt_path}")
        state = torch.load(ckpt_path, map_location="cpu")
        # 유연 로딩 (키 불일치 무시)
        missing, unexpected = model.load_state_dict(state, strict=False)
        if missing:
            print(f"[Warn] Missing keys: {len(missing)} (showing first 5): {missing[:5]}")
        if unexpected:
            print(f"[Warn] Unexpected keys: {len(unexpected)} (showing first 5): {unexpected[:5]}")
    model.to(device).eval()
    return model


# ============== Tokenization (encoder) ==============
def tokenize_sources(
    texts: List[str],
    enc_name: str = "beomi/kcbert-base",
    max_length: int = 192,
    device: str = "cuda",
):
    enc_tok = BertTokenizer.from_pretrained(enc_name)
    batch = enc_tok(
        texts,
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors="pt",
    )
    return batch["input_ids"].to(device), batch["attention_mask"].to(device)


# ============== Decoding ==============
@torch.no_grad()
def greedy_decode(
    model: CorrectionModel,
    dec_tok: T5TokenizerFast,
    enc_input_ids: torch.Tensor,         # (B, S)
    enc_attention_mask: torch.Tensor,    # (B, S)
    max_new_tokens: int = 128,
) -> List[str]:
    device = enc_input_ids.device
    bos_id = dec_tok.bos_token_id
    pad_id = dec_tok.pad_token_id
    eos_id = dec_tok.eos_token_id

    B = enc_input_ids.size(0)
    dec_ids = torch.full((B, 1), bos_id, dtype=torch.long, device=device)
    dec_attn = torch.ones_like(dec_ids)

    finished = torch.zeros(B, dtype=torch.bool, device=device)

    for _ in range(max_new_tokens):
        logits = model(
            input_ids=enc_input_ids,
            attention_mask=enc_attention_mask,
            decoder_input_ids=dec_ids,
            decoder_attention_mask=dec_attn,
        )  # (B, T, V)
        next_id = logits[:, -1, :].argmax(-1, keepdim=True)  # (B, 1)

        dec_ids = torch.cat([dec_ids, next_id], dim=1)
        dec_attn = torch.cat([dec_attn, torch.ones_like(next_id)], dim=1)

        finished = finished | (next_id.squeeze(1) == eos_id)
        if finished.all():
            break

    # BOS 제거하고 디코딩
    outs = []
    for i in range(B):
        seq = dec_ids[i, 1:].tolist()
        outs.append(dec_tok.decode(seq, skip_special_tokens=True))
    return outs


@torch.no_grad()
def beam_search_decode(
    model: CorrectionModel,
    dec_tok: T5TokenizerFast,
    enc_input_ids: torch.Tensor,        # (B, S)
    enc_attention_mask: torch.Tensor,   # (B, S)
    max_new_tokens: int = 128,
    num_beams: int = 4,
    length_penalty: float = 1.0,
) -> List[str]:
    """
    간단한 배치별 독립 beam search (메모리 절약형, 기능 최소)
    - 각 배치 샘플을 독립적으로 beam 탐색
    - 속도/효율 위해 너무 복잡하게 안 함 (실전은 HF generate 권장)
    """
    device = enc_input_ids.device
    bos_id = dec_tok.bos_token_id
    eos_id = dec_tok.eos_token_id

    B = enc_input_ids.size(0)
    results = []

    for b in range(B):
        enc_ids = enc_input_ids[b:b+1]
        enc_mask = enc_attention_mask[b:b+1]

        # 초기 beam
        beams = [(torch.tensor([[bos_id]], device=device, dtype=torch.long),
                  torch.tensor([[1]], device=device, dtype=torch.long),
                  0.0,  # logprob
                  False)]  # finished

        for _ in range(max_new_tokens):
            new_beams = []
            all_finished = True
            for dec_ids, dec_attn, logp, finished in beams:
                if finished:
                    new_beams.append((dec_ids, dec_attn, logp, True))
                    continue
                all_finished = False

                logits = model(
                    input_ids=enc_ids,
                    attention_mask=enc_mask,
                    decoder_input_ids=dec_ids,
                    decoder_attention_mask=dec_attn,
                )  # (1, T, V)
                next_logprobs = torch.log_softmax(logits[:, -1, :], dim=-1).squeeze(0)  # (V,)

                # 상위 num_beams 후보 확장
                topk_logp, topk_ids = torch.topk(next_logprobs, k=num_beams)
                for lp, tid in zip(topk_logp.tolist(), topk_ids.tolist()):
                    tid = int(tid)
                    nd = torch.cat([dec_ids, torch.tensor([[tid]], device=device)], dim=1)
                    na = torch.cat([dec_attn, torch.ones((1,1), device=device, dtype=torch.long)], dim=1)
                    fin = (tid == eos_id)
                    new_beams.append((nd, na, logp + lp, fin))

            # beam 정렬/상위 유지 (간단 length_penalty)
            def score_fn(tup):
                nd, _, logp, fin = tup
                length = nd.size(1) - 1  # exclude BOS
                lp = (length ** length_penalty) if length > 0 else 1.0
                return logp / lp

            new_beams.sort(key=score_fn, reverse=True)
            beams = new_beams[:num_beams]

            if all_finished:
                break

        # 최종 선택
        best = max(beams, key=score_fn)
        seq = best[0][0, 1:].tolist()  # drop BOS
        text = dec_tok.decode(seq, skip_special_tokens=True)
        results.append(text)

    return results


# ============== Main ==============
def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--encoder_name", type=str, default="beomi/kcbert-base")
    ap.add_argument("--spm_path", type=str, required=True, help="T5TokenizerFast vocab_file (SentencePiece .model)")
    ap.add_argument("--ckpt", type=str, default=None, help="optional model state_dict .pt")
    ap.add_argument("--max_src_len", type=int, default=192)
    ap.add_argument("--max_new_tokens", type=int, default=128)
    ap.add_argument("--beam", type=int, default=1, help="1: greedy, >=2: beam size")
    ap.add_argument("--length_penalty", type=float, default=1.0)
    ap.add_argument("--input", type=str, nargs="*", help="원문 문장(스페이스로 여러 개). 파일을 쓰려면 --file")
    ap.add_argument("--file", type=str, default=None, help="원문 텍스트 파일 경로 (newline로 여러 문장)")
    return ap.parse_args()


def main():
    args = parse_args()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[Info] device: {device}")

    dec_tok = load_decoder_tokenizer(args.spm_path, legacy=False)
    print(f"[Info] Decoder vocab size: {dec_tok.vocab_size}")
    print(f"[Info] PAD/BOS/EOS = {dec_tok.pad_token_id}/{getattr(dec_tok,'bos_token_id',None)}/{dec_tok.eos_token_id}")

    # 입력 수집
    texts: List[str] = []
    if args.file:
        with open(args.file, "r", encoding="utf-8") as f:
            texts = [ln.strip() for ln in f if ln.strip()]
    elif args.input:
        texts = args.input
    else:
        # 인터랙티브 하나
        s = input("원문을 입력하세요: ").strip()
        texts = [s]

    # 토크나이즈(인코더)
    enc_input_ids, enc_attention_mask = tokenize_sources(
        texts, enc_name=args.encoder_name, max_length=args.max_src_len, device=str(device).split(":")[0]
    )

    # 모델
    model = build_model(args.encoder_name, dec_tok, args.ckpt, device)

    # 디코딩
    if args.beam >= 2:
        outs = beam_search_decode(
            model, dec_tok,
            enc_input_ids, enc_attention_mask,
            max_new_tokens=args.max_new_tokens,
            num_beams=args.beam,
            length_penalty=args.length_penalty,
        )
        mode = f"beam({args.beam})"
    else:
        outs = greedy_decode(
            model, dec_tok,
            enc_input_ids, enc_attention_mask,
            max_new_tokens=args.max_new_tokens,
        )
        mode = "greedy"

    # 출력
    for i, (src, hyp) in enumerate(zip(texts, outs)):
        print("\n==============================")
        print(f"[SRC] {src}")
        print(f"[{mode.upper()}] {hyp}")


if __name__ == "__main__":
    main()

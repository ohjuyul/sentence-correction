# train_full_split.py
# 2.23M 스트리밍 + 8:1:1 분할 + tqdm + 에폭별 val(loss/acc/CER) + EarlyStopping(patience=3)
# 디코더 우선 학습 → plateau 시 인코더 점진적 언프리즈(뒤에서부터 k층씩)

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import math
from dataclasses import dataclass
from typing import Dict, Optional, Iterator

import torch
import torch.nn as nn
from torch.utils.data import IterableDataset, DataLoader
from transformers import T5TokenizerFast, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup
from tqdm.auto import tqdm

from model3 import CorrectionModel
from transformers import BertTokenizer, PreTrainedTokenizerBase

# ========= 사용자 경로/고정값 =========
ENCODER_NAME    = "beomi/kcbert-base"
DECODER_SPM     = "../spm_out/tokenizer_32k.model"
KO_PATH         = "../ko.txt"
COR_PATH        = "../correct.txt"
TOTAL_LINES     = 2_228_281   # 제공한 정확한 총 샘플 수

# ========= 데이터/로더 설정 =========
ENC_MAX_LEN     = 300
DEC_MAX_LEN     = 256
BATCH_SIZE      = 8
NUM_WORKERS     = 2
ACCUM_STEPS     = 1
PIN_MEMORY      = torch.cuda.is_available()

# ========= 학습 설정 =========
EPOCHS          = 30
LR_DEC          = 3e-4
LR_ENC          = 1e-5
WEIGHT_DECAY    = 0.01
WARMUP_STEPS    = 10_000
USE_COSINE      = True
LABEL_SMOOTHING = 0.1
GRAD_CLIP_NORM  = 1.0

# ========= 언프리즈/플래토 설정 =========
UNFREEZE_START_LAYERS = 0
UNFREEZE_STEP_LAYERS  = 2
UNFREEZE_MAX_LAYERS   = 12
PATIENCE_STEPS_TRAIN  = 5_000  # 훈련 EMA plateaus 감지 간격(스텝)
MIN_IMPROVE_RATIO     = 0.001
EMA_BETA              = 0.98

# ========= Early Stopping (Validation 기반) =========
ES_MONITOR     = "val_loss"   # "val_loss" 또는 "val_acc"
ES_MIN_DELTA   = 0.0          # 개선으로 인정할 최소 변화량
ES_PATIENCE_EP = 3            # 연속 N 에폭 개선 없으면 중단

# ========= 저장/로그 =========
CKPT_DIR   = "./checkpoints"
LOG_EVERY  = 100                   # step
MILESTONE_EVERY_EPOCHS = 3         # 3에폭마다 마일스톤 저장
SAVE_EVERY_STEPS = 20_000          # (옵션) 스텝 주기 저장
os.makedirs(CKPT_DIR, exist_ok=True)

# ========= 평가 옵션 =========
# CER 속도 위해 캡 (0이면 전체 샘플 CER 계산)
EVAL_CER_MAX_SAMPLES = 5_000

IGNORE_INDEX = -100


# --------------------------
# 토크나이저 유틸
# --------------------------
def ensure_special_tokens(tok: T5TokenizerFast) -> T5TokenizerFast:
    changed = False
    if tok.pad_token is None:
        tok.add_special_tokens({"pad_token": "<pad>"}); changed = True
    if getattr(tok, "bos_token", None) is None:
        tok.add_special_tokens({"bos_token": "<s>"}); changed = True
    if tok.eos_token is None:
        tok.add_special_tokens({"eos_token": "</s>"}); changed = True
    if changed:
        print("[Info] Added missing special tokens to decoder tokenizer.")
    return tok

def build_dec_tok(spm_path: str, legacy: bool=False) -> T5TokenizerFast:
    if not os.path.exists(spm_path):
        raise FileNotFoundError(spm_path)
    tok = T5TokenizerFast(vocab_file=spm_path, extra_ids=0, legacy=legacy)
    return ensure_special_tokens(tok)


# --------------------------
# 스트리밍 분할 데이터셋 (메모리 미적재)
# --------------------------
def shift_right_1d(labels_1d: torch.Tensor, bos_id: int) -> torch.Tensor:
    base = labels_1d.masked_fill(labels_1d.eq(IGNORE_INDEX), 0)
    out = base.clone()
    if out.numel() > 0:
        out[1:] = base[:-1]
        out[0] = bos_id
    return out

class SplitStreamDataset(IterableDataset):
    def __init__(
        self,
        ko_path: str,
        cor_path: str,
        start_idx: int,   # 포함
        end_idx: int,     # 미포함
        enc_tok_name: str = ENCODER_NAME,
        enc_max_len: int = ENCOD_MAX_LEN,
        dec_tok: Optional[PreTrainedTokenizerBase] = None,
        dec_max_len: int = DEC_MAX_LEN,
    ):
        super().__init__()
        self.ko_path = ko_path
        self.cor_path = cor_path
        self.start_idx = start_idx
        self.end_idx = end_idx
        self.enc_tok = BertTokenizer.from_pretrained(enc_tok_name)
        if dec_tok is None:
            raise ValueError("Need decoder tokenizer")
        self.dec_tok = dec_tok
        self.enc_max_len = enc_max_len
        self.dec_max_len = dec_max_len

    def _tok_src(self, text: str):
        return self.enc_tok(
            text, max_length=self.enc_max_len, truncation=True,
            padding="max_length", return_tensors="pt"
        )

    def _tok_tgt_pack(self, text: str):
        enc = self.dec_tok(
            text, max_length=self.dec_max_len, truncation=True,
            padding="max_length", return_tensors="pt"
        )
        tgt_ids = enc["input_ids"][0]
        pad_id  = self.dec_tok.pad_token_id
        bos_id  = self.dec_tok.bos_token_id

        labels = tgt_ids.clone()
        labels[labels == pad_id] = IGNORE_INDEX

        dec_inp = shift_right_1d(labels, bos_id)
        dec_attn = (dec_inp != pad_id).long()
        return {
            "labels": labels,
            "decoder_input_ids": dec_inp,
            "decoder_attention_mask": dec_attn,
        }

    def __iter__(self) -> Iterator[Dict[str, torch.Tensor]]:
        with open(self.ko_path, "r", encoding="utf-8") as f_ko, open(self.cor_path, "r", encoding="utf-8") as f_cor:
            for i, (src, tgt) in enumerate(zip(f_ko, f_cor)):
                if i < self.start_idx:
                    continue
                if i >= self.end_idx:
                    break
                src = (src or "").strip()
                tgt = (tgt or "").strip()
                if not src or not tgt:
                    continue
                sp = self._tok_src(src)
                tp = self._tok_tgt_pack(tgt)
                yield {
                    "input_ids": sp["input_ids"][0],
                    "attention_mask": sp["attention_mask"][0],
                    "labels": tp["labels"],
                    "decoder_input_ids": tp["decoder_input_ids"],
                    "decoder_attention_mask": tp["decoder_attention_mask"],
                }

    def __len__(self) -> int:
        return max(0, self.end_idx - self.start_idx)


# --------------------------
# 분할 정의(8:1:1)
# --------------------------
def make_splits(n_total: int):
    n_train = int(n_total * 0.8)
    n_val   = int(n_total * 0.1)
    n_test  = n_total - n_train - n_val
    idxs = {
        "train": (0, n_train),
        "val":   (n_train, n_train + n_val),
        "test":  (n_train + n_val, n_total),
    }
    sizes = {"train": n_train, "val": n_val, "test": n_test}
    return idxs, sizes


# --------------------------
# 모델/옵티마이저/스케줄러/손실
# --------------------------
def build_model(dec_tok: T5TokenizerFast, device) -> CorrectionModel:
    m = CorrectionModel(
        encoder_name=ENCODER_NAME,
        num_decoder_layers=6,
        decoder_tokenizer=dec_tok,
    )
    m.to(device)
    return m

def split_params_for_groups(model: CorrectionModel):
    enc_decay, enc_nodecay, dec_decay, dec_nodecay = [], [], [], []
    for n, p in model.encoder.named_parameters():
        (enc_nodecay if any(x in n for x in ["bias","LayerNorm.weight","layer_norm.weight","layernorm.weight"]) else enc_decay).append(p)
    for n, p in model.decoder.named_parameters():
        (dec_nodecay if any(x in n for x in ["bias","LayerNorm.weight","layer_norm.weight","layernorm.weight"]) else dec_decay).append(p)
    for n, p in model.output_projection.named_parameters():
        (dec_nodecay if any(x in n for x in ["bias","LayerNorm.weight","layer_norm.weight","layernorm.weight"]) else dec_decay).append(p)
    return enc_decay, enc_nodecay, dec_decay, dec_nodecay

def build_optimizer(model: CorrectionModel):
    enc_d, enc_n, dec_d, dec_n = split_params_for_groups(model)
    return torch.optim.AdamW(
        [
            {"params": enc_d, "weight_decay": WEIGHT_DECAY, "lr": LR_ENC},
            {"params": enc_n, "weight_decay": 0.0,          "lr": LR_ENC},
            {"params": dec_d, "weight_decay": WEIGHT_DECAY, "lr": LR_DEC},
            {"params": dec_n, "weight_decay": 0.0,          "lr": LR_DEC},
        ],
        betas=(0.9, 0.98), eps=1e-8
    )

def steps_per_epoch(n_samples: int):
    return math.ceil(n_samples / (BATCH_SIZE * max(1, ACCUM_STEPS)))

def build_scheduler(optimizer, total_steps: int):
    if USE_COSINE:
        return get_cosine_schedule_with_warmup(optimizer, WARMUP_STEPS, total_steps)
    else:
        return get_linear_schedule_with_warmup(optimizer, WARMUP_STEPS, total_steps)

class LabelSmoothingCE(nn.Module):
    def __init__(self, smoothing: float, ignore_index: int = -100):
        super().__init__()
        self.smoothing = smoothing
        self.ignore_index = ignore_index
    def forward(self, logits, targets):
        B, T, V = logits.shape
        pad_mask = targets.eq(self.ignore_index)
        n_valid = (~pad_mask).sum().clamp_min(1)
        if self.smoothing > 0.0:
            log_probs = torch.log_softmax(logits, dim=-1)
            nll = -log_probs.gather(dim=-1, index=targets.masked_fill(pad_mask, 0).unsqueeze(-1)).squeeze(-1)
            nll = nll.masked_fill(pad_mask, 0.0).sum() / n_valid
            smooth = -log_probs.mean(dim=-1)
            smooth = smooth.masked_fill(pad_mask, 0.0).sum() / n_valid
            return (1 - self.smoothing) * nll + self.smoothing * smooth
        else:
            return nn.CrossEntropyLoss(ignore_index=self.ignore_index)(logits.view(B*T, V), targets.view(B*T))

criterion = LabelSmoothingCE(LABEL_SMOOTHING, ignore_index=IGNORE_INDEX)


# --------------------------
# 언프리즈/학습 상태
# --------------------------
def set_encoder_trainable_layers(model: CorrectionModel, k_last_layers: int):
    for p in model.encoder.parameters():
        p.requires_grad = False
    if k_last_layers > 0:
        layers = list(model.encoder.encoder.layer)
        start = max(0, len(layers) - k_last_layers)
        for i, layer in enumerate(layers):
            for p in layer.parameters():
                p.requires_grad = (i >= start)
    for p in model.encoder.embeddings.parameters():
        p.requires_grad = False
    if hasattr(model.encoder, "pooler") and model.encoder.pooler is not None:
        for p in model.encoder.pooler.parameters():
            p.requires_grad = False

@dataclass
class TrainState:
    global_step: int = 0
    ema_loss: Optional[float] = None
    best_train_ema: float = float("inf")
    steps_since_improve: int = 0
    unfrozen_layers: int = UNFREEZE_START_LAYERS

@dataclass
class ESState:
    best_metric: float = float("inf") if ES_MONITOR=="val_loss" else -float("inf")
    best_epoch: int = -1
    wait: int = 0

def is_better_val(curr, best):
    if ES_MONITOR == "val_loss":
        return curr < (best - ES_MIN_DELTA)
    else:
        return curr > (best + ES_MIN_DELTA)


# --------------------------
# 평가 헬퍼 (ids→텍스트, CER)
# --------------------------
def ids_to_text(dec_tok, ids: torch.Tensor, pad_id: int, eos_id: int) -> str:
    arr = ids.detach().cpu().tolist()
    arr = [pad_id if t == IGNORE_INDEX else t for t in arr]
    if eos_id in arr:
        cut = arr.index(eos_id) + 1  # eos 포함 후 디코드
        arr = arr[:cut]
    return dec_tok.decode(arr, skip_special_tokens=True, clean_up_tokenization_spaces=True)

def cer_one(ref: str, hyp: str) -> float:
    R, H = ref, hyp
    n, m = len(R), len(H)
    if n == 0:
        return float(m > 0)
    dp = [[0]*(m+1) for _ in range(n+1)]
    for i in range(n+1):
        dp[i][0] = i
    for j in range(m+1):
        dp[0][j] = j
    for i in range(1, n+1):
        ri = R[i-1]
        for j in range(1, m+1):
            hj = H[j-1]
            dp[i][j] = min(
                dp[i-1][j] + 1,
                dp[i][j-1] + 1,
                dp[i-1][j-1] + (ri != hj)
            )
    return dp[n][m] / max(1, n)


# --------------------------
# 평가 루프 (teacher-forced)
#   - loss
#   - token accuracy (pad 제외)
#   - CER (문자단위, cap 적용)
# --------------------------
@torch.no_grad()
def evaluate(model: CorrectionModel, loader: DataLoader, device) -> Dict[str, float]:
    model.eval()
    total_loss = 0.0
    total_tok  = 0
    total_cor  = 0
    total_cer  = 0.0
    n_cer      = 0

    dec_tok  = model.dec_tok
    pad_id   = dec_tok.pad_token_id
    eos_id   = dec_tok.eos_token_id

    for batch in tqdm(loader, desc="Validate/Test", leave=False):
        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}
        logits = model(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
            decoder_input_ids=batch["decoder_input_ids"],
            decoder_attention_mask=batch["decoder_attention_mask"],
            labels=None,
        )
        loss = criterion(logits, batch["labels"]).item()
        total_loss += loss

        # token-level accuracy
        preds = logits.argmax(-1)     # (B,T)
        golds = batch["labels"]       # (B,T)
        mask  = golds.ne(IGNORE_INDEX)
        total_tok += mask.sum().item()
        total_cor += (preds.eq(golds) & mask).sum().item()

        # CER (cap 적용)
        if EVAL_CER_MAX_SAMPLES == 0 or n_cer < EVAL_CER_MAX_SAMPLES:
            B = preds.size(0)
            for b in range(B):
                if EVAL_CER_MAX_SAMPLES and n_cer >= EVAL_CER_MAX_SAMPLES:
                    break
                hyp_text  = ids_to_text(dec_tok, preds[b], pad_id, eos_id)
                gold_text = ids_to_text(dec_tok, golds[b], pad_id, eos_id)
                total_cer += cer_one(gold_text, hyp_text)
                n_cer     += 1

    avg_loss = total_loss / max(1, len(loader))
    acc = (total_cor / max(1, total_tok)) if total_tok > 0 else 0.0
    avg_cer = (total_cer / max(1, n_cer)) if n_cer > 0 else 0.0
    return {"loss": avg_loss, "acc": acc, "cer": avg_cer}


# --------------------------
# 저장 유틸
# --------------------------
def save_ckpt(model: CorrectionModel, tag: str, step: int):
    path = os.path.join(CKPT_DIR, f"step_{step}_{tag}.pt")
    torch.save(model.state_dict(), path)
    print(f"[Save] {path}")


# --------------------------
# 메인 학습 루프
# --------------------------
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[Info] device: {device}")

    # 분할 인덱스/크기
    (idxs, sizes) = make_splits(TOTAL_LINES)
    n_train, n_val, n_test = sizes["train"], sizes["val"], sizes["test"]
    print(f"[Split] train={n_train:,}, val={n_val:,}, test={n_test:,}")

    steps_tr_ep = steps_per_epoch(n_train)
    total_steps = steps_tr_ep * EPOCHS
    print(f"[Info] steps/epoch(train)={steps_tr_ep}, total_steps(est)={total_steps}")

    # 토크나이저/모델
    dec_tok = build_dec_tok(DECODER_SPM, legacy=False)
    print(f"[Info] Decoder vocab: {dec_tok.vocab_size}, PAD/BOS/EOS={dec_tok.pad_token_id}/{dec_tok.bos_token_id}/{dec_tok.eos_token_id}")

    model = build_model(dec_tok, device)
    set_encoder_trainable_layers(model, UNFREEZE_START_LAYERS)
    optimizer = build_optimizer(model)
    scheduler = build_scheduler(optimizer, total_steps)
    scaler = torch.amp.GradScaler(device="cuda", enabled=torch.cuda.is_available())

    # 로더들
    def build_loader_for(split: str, bs=BATCH_SIZE, nw=NUM_WORKERS):
        s, e = idxs[split]
        ds = SplitStreamDataset(KO_PATH, COR_PATH, start_idx=s, end_idx=e,
                                enc_tok_name=ENCODER_NAME, enc_max_len=ENC_MAX_LEN,
                                dec_tok=dec_tok, dec_max_len=DEC_MAX_LEN)
        return DataLoader(ds, batch_size=bs, shuffle=False, num_workers=nw, pin_memory=PIN_MEMORY)

    train_loader = build_loader_for("train")
    val_loader   = build_loader_for("val")
    test_loader  = build_loader_for("test")

    state = TrainState()
    es    = ESState()

    for epoch in range(EPOCHS):
        # ===== Train =====
        model.train()
        running = 0.0
        pbar = tqdm(total=steps_tr_ep, desc=f"Epoch {epoch+1}/{EPOCHS} [train]", leave=True)
        accum = 0

        # 매 epoch마다 새 iterator 필요(IterableDataset)
        train_iter = iter(train_loader)

        while state.global_step // max(1, ACCUM_STEPS) < (epoch+1)*steps_tr_ep:
            try:
                batch = next(train_iter)
            except StopIteration:
                train_iter = iter(build_loader_for("train"))
                batch = next(train_iter)

            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}
            with torch.amp.autocast(device_type="cuda", dtype=torch.float16, enabled=torch.cuda.is_available()):
                logits = model(
                    input_ids=batch["input_ids"],
                    attention_mask=batch["attention_mask"],
                    decoder_input_ids=batch["decoder_input_ids"],
                    decoder_attention_mask=batch["decoder_attention_mask"],
                    labels=None,
                )
                loss = criterion(logits, batch["labels"]) / ACCUM_STEPS

            scaler.scale(loss).backward()
            accum += 1

            if accum >= ACCUM_STEPS:
                if GRAD_CLIP_NORM and GRAD_CLIP_NORM > 0:
                    scaler.unscale_(optimizer)
                    nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad(set_to_none=True)
                scheduler.step()

                state.global_step += 1
                accum = 0

                # EMA/로그
                l = loss.item() * ACCUM_STEPS
                running += l
                if state.ema_loss is None:
                    state.ema_loss = l
                else:
                    state.ema_loss = EMA_BETA * state.ema_loss + (1 - EMA_BETA) * l

                # tqdm
                lr_now = optimizer.param_groups[2]["lr"]
                pbar.update(1)
                pbar.set_postfix({"loss": f"{l:.2f}", "ema": f"{state.ema_loss:.2f}", "lr": f"{lr_now:.2e}", "unfrz": state.unfrozen_layers})

                if state.global_step % LOG_EVERY == 0:
                    avg = running / LOG_EVERY
                    print(f"[train] ep {epoch+1} step {state.global_step} | avg_loss {avg:.4f} | ema {state.ema_loss:.4f} | lr {lr_now:.2e} | unfrz {state.unfrozen_layers}")
                    running = 0.0

                if SAVE_EVERY_STEPS and state.global_step % SAVE_EVERY_STEPS == 0:
                    save_ckpt(model, "autosave", state.global_step)

                # 훈련 plateau → 인코더 점진 언프리즈
                improved = (state.ema_loss + MIN_IMPROVE_RATIO * state.best_train_ema) < state.best_train_ema
                if improved:
                    state.best_train_ema = state.ema_loss
                    state.steps_since_improve = 0
                else:
                    state.steps_since_improve += 1
                    if state.steps_since_improve >= PATIENCE_STEPS_TRAIN and state.unfrozen_layers < UNFREEZE_MAX_LAYERS:
                        new_layers = min(UNFREEZE_MAX_LAYERS, state.unfrozen_layers + UNFREEZE_STEP_LAYERS)
                        print(f"[Unfreeze] plateau → encoder last {UNFREEZE_STEP_LAYERS} layers: {state.unfrozen_layers} → {new_layers}")
                        state.unfrozen_layers = new_layers
                        set_encoder_trainable_layers(model, state.unfrozen_layers)
                        state.steps_since_improve = 0

        pbar.close()

        # ===== Validate =====
        val_metrics = evaluate(model, val_loader, device)
        print(f"[val] epoch {epoch+1} | loss {val_metrics['loss']:.4f} | acc {val_metrics['acc']*100:.2f}% | cer {val_metrics['cer']:.4f}")

        # 에폭마다 저장 + 3에폭마다 마일스톤 저장
        save_ckpt(model, f"epoch{epoch+1}", state.global_step)
        if (epoch + 1) % MILESTONE_EVERY_EPOCHS == 0:
            save_ckpt(model, f"epoch{epoch+1}_milestone", state.global_step)

        # ===== Early Stopping 판단 =====
        monitor_value = val_metrics["loss"] if ES_MONITOR == "val_loss" else val_metrics["acc"]
        if is_better_val(monitor_value, es.best_metric):
            es.best_metric = monitor_value
            es.best_epoch = epoch + 1
            es.wait = 0
            save_ckpt(model, f"best_{ES_MONITOR}", state.global_step)
        else:
            es.wait += 1
            if es.wait >= ES_PATIENCE_EP:
                print(f"[EarlyStop] No improvement in {ES_MONITOR} for {ES_PATIENCE_EP} epochs (best at epoch {es.best_epoch}).")
                break

    # ===== Test (마지막) =====
    test_metrics = evaluate(model, test_loader, device)
    print(f"[test] loss {test_metrics['loss']:.4f} | acc {test_metrics['acc']*100:.2f}% | cer {test_metrics['cer']:.4f}")
    print("[Done]")

if __name__ == "__main__":
    main()

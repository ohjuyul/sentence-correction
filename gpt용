# train_full_split.py
# 2.23M 스트리밍 + 8:1:1 분할(파일 분리 버전) + tqdm + 에폭별 val(loss/acc/CER) + EarlyStopping(patience=3)
# 디코더 우선 학습 → plateau 시 인코더 점진적 언프리즈(뒤에서부터 k층씩)

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import math
from dataclasses import dataclass
from typing import Dict, Optional

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import T5TokenizerFast, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup
from tqdm.auto import tqdm

from model3 import CorrectionModel
from data_txt_stream import GrammarCorrectionDataset  # ✅ 기존 Dataset 그대로 사용

# ========= 경로/고정값 =========
ENCODER_NAME = "beomi/kcbert-base"
DECODER_SPM  = "../spm_out/tokenizer_32k.model"

# 이미 8:1:1로 분할 저장된 파일들 (순서 보존, 라인 1:1)
KO_TRAIN_PATH = "../ko.train.txt"
KO_VAL_PATH   = "../ko.val.txt"
KO_TEST_PATH  = "../ko.test.txt"

COR_TRAIN_PATH = "../correct.train.txt"
COR_VAL_PATH   = "../correct.val.txt"
COR_TEST_PATH  = "../correct.test.txt"

# 총 라인수 및 분할 개수 (고정)
TOTAL_LINES  = 2_228_281
TRAIN_LINES  = 1_782_624
VAL_LINES    = 222_828
TEST_LINES   = 222_829

# ========= 데이터/로더 설정 =========
ENC_MAX_LEN     = 300
DEC_MAX_LEN     = 256
BATCH_SIZE      = 8
NUM_WORKERS     = 2
ACCUM_STEPS     = 1
PIN_MEMORY      = torch.cuda.is_available()

# ========= 학습 설정 =========
EPOCHS          = 30
LR_DEC          = 3e-4
LR_ENC          = 1e-5
WEIGHT_DECAY    = 0.01
WARMUP_STEPS    = 10_000
USE_COSINE      = True
LABEL_SMOOTHING = 0.1
GRAD_CLIP_NORM  = 1.0

# ========= 언프리즈/플래토 설정 =========
UNFREEZE_START_LAYERS = 0
UNFREEZE_STEP_LAYERS  = 2
UNFREEZE_MAX_LAYERS   = 12
PATIENCE_STEPS_TRAIN  = 5_000
MIN_IMPROVE_RATIO     = 0.001
EMA_BETA              = 0.98

# ========= Early Stopping (Validation 기반) =========
ES_MONITOR     = "val_loss"   # "val_loss" 또는 "val_acc"
ES_MIN_DELTA   = 0.0
ES_PATIENCE_EP = 3

# ========= 저장/로그 =========
CKPT_DIR   = "./checkpoints"
MILESTONE_EVERY_EPOCHS = 3
SAVE_EVERY_STEPS = 20_000
os.makedirs(CKPT_DIR, exist_ok=True)

# ========= 평가 옵션 =========
# CER 속도 위해 cap (0이면 전체 샘플 계산)
EVAL_CER_MAX_SAMPLES = 5_000

IGNORE_INDEX = -100

# --------------------------
# 토크나이저 유틸
# --------------------------
def ensure_special_tokens(tok: T5TokenizerFast) -> T5TokenizerFast:
    changed = False
    if tok.pad_token is None:
        tok.add_special_tokens({"pad_token": "<pad>"}); changed = True
    if getattr(tok, "bos_token", None) is None:
        tok.add_special_tokens({"bos_token": "<s>"}); changed = True
    if tok.eos_token is None:
        tok.add_special_tokens({"eos_token": "</s>"}); changed = True
    if changed:
        print("[Info] Added missing special tokens to decoder tokenizer.")
    return tok

def build_dec_tok(spm_path: str, legacy: bool=False) -> T5TokenizerFast:
    if not os.path.exists(spm_path):
        raise FileNotFoundError(spm_path)
    tok = T5TokenizerFast(vocab_file=spm_path, extra_ids=0, legacy=legacy)
    return ensure_special_tokens(tok)

# --------------------------
# 모델/옵티마이저/스케줄러/손실
# --------------------------
def build_model(dec_tok: T5TokenizerFast, device) -> CorrectionModel:
    m = CorrectionModel(
        encoder_name=ENCODER_NAME,
        num_decoder_layers=6,
        decoder_tokenizer=dec_tok,
    )
    m.to(device)
    return m

def split_params_for_groups(model: CorrectionModel):
    enc_decay, enc_nodecay, dec_decay, dec_nodecay = [], [], [], []
    for n, p in model.encoder.named_parameters():
        (enc_nodecay if any(x in n for x in ["bias","LayerNorm.weight","layer_norm.weight","layernorm.weight"]) else enc_decay).append(p)
    for n, p in model.decoder.named_parameters():
        (dec_nodecay if any(x in n for x in ["bias","LayerNorm.weight","layer_norm.weight","layernorm.weight"]) else dec_decay).append(p)
    for n, p in model.output_projection.named_parameters():
        (dec_nodecay if any(x in n for x in ["bias","LayerNorm.weight","layer_norm.weight","layernorm.weight"]) else dec_decay).append(p)
    return enc_decay, enc_nodecay, dec_decay, dec_nodecay

def build_optimizer(model: CorrectionModel):
    enc_d, enc_n, dec_d, dec_n = split_params_for_groups(model)
    return torch.optim.AdamW(
        [
            {"params": enc_d, "weight_decay": WEIGHT_DECAY, "lr": LR_ENC},
            {"params": enc_n, "weight_decay": 0.0,          "lr": LR_ENC},
            {"params": dec_d, "weight_decay": WEIGHT_DECAY, "lr": LR_DEC},
            {"params": dec_n, "weight_decay": 0.0,          "lr": LR_DEC},
        ],
        betas=(0.9, 0.98), eps=1e-8
    )

def steps_per_epoch(n_samples: int):
    return math.ceil(n_samples / (BATCH_SIZE * max(1, ACCUM_STEPS)))

def build_scheduler(optimizer, total_steps: int):
    if USE_COSINE:
        return get_cosine_schedule_with_warmup(optimizer, WARMUP_STEPS, total_steps)
    else:
        return get_linear_schedule_with_warmup(optimizer, WARMUP_STEPS, total_steps)

class LabelSmoothingCE(nn.Module):
    def __init__(self, smoothing: float, ignore_index: int = -100):
        super().__init__()
        self.smoothing = smoothing
        self.ignore_index = ignore_index
    def forward(self, logits, targets):
        B, T, V = logits.shape
        pad_mask = targets.eq(self.ignore_index)
        n_valid = (~pad_mask).sum().clamp_min(1)
        if self.smoothing > 0.0:
            log_probs = torch.log_softmax(logits, dim=-1)
            nll = -log_probs.gather(dim=-1, index=targets.masked_fill(pad_mask, 0).unsqueeze(-1)).squeeze(-1)
            nll = nll.masked_fill(pad_mask, 0.0).sum() / n_valid
            smooth = -log_probs.mean(dim=-1)
            smooth = smooth.masked_fill(pad_mask, 0.0).sum() / n_valid
            return (1 - self.smoothing) * nll + self.smoothing * smooth
        else:
            return nn.CrossEntropyLoss(ignore_index=self.ignore_index)(logits.view(B*T, V), targets.view(B*T))

criterion = LabelSmoothingCE(LABEL_SMOOTHING, ignore_index=IGNORE_INDEX)

# --------------------------
# 언프리즈/학습 상태
# --------------------------
def set_encoder_trainable_layers(model: CorrectionModel, k_last_layers: int):
    for p in model.encoder.parameters():
        p.requires_grad = False
    if k_last_layers > 0:
        layers = list(model.encoder.encoder.layer)
        start = max(0, len(layers) - k_last_layers)
        for i, layer in enumerate(layers):
            for p in layer.parameters():
                p.requires_grad = (i >= start)
    for p in model.encoder.embeddings.parameters():
        p.requires_grad = False
    if hasattr(model.encoder, "pooler") and model.encoder.pooler is not None:
        for p in model.encoder.pooler.parameters():
            p.requires_grad = False

@dataclass
class TrainState:
    global_step: int = 0
    ema_loss: Optional[float] = None
    best_train_ema: float = float("inf")
    steps_since_improve: int = 0
    unfrozen_layers: int = UNFREEZE_START_LAYERS

@dataclass
class ESState:
    best_metric: float = float("inf") if ES_MONITOR=="val_loss" else -float("inf")
    best_epoch: int = -1
    wait: int = 0

def is_better(curr, best):
    return (curr < best) if ES_MONITOR=="val_loss" else (curr > best)

# --------------------------
# 평가 헬퍼 (ids→텍스트, CER)
# --------------------------
def ids_to_text(dec_tok, ids: torch.Tensor, pad_id: int, eos_id: int) -> str:
    arr = ids.detach().cpu().tolist()
    arr = [pad_id if t == IGNORE_INDEX else t for t in arr]
    if eos_id in arr:
        cut = arr.index(eos_id) + 1  # eos 포함 후 디코드
        arr = arr[:cut]
    return dec_tok.decode(arr, skip_special_tokens=True, clean_up_tokenization_spaces=True)

def cer_one(ref: str, hyp: str) -> float:
    R, H = ref, hyp
    n, m = len(R), len(H)
    if n == 0:
        return float(m > 0)
    dp = [[0]*(m+1) for _ in range(n+1)]
    for i in range(n+1):
        dp[i][0] = i
    for j in range(m+1):
        dp[0][j] = j
    for i in range(1, n+1):
        ri = R[i-1]
        for j in range(1, m+1):
            hj = H[j-1]
            dp[i][j] = min(
                dp[i-1][j] + 1,
                dp[i][j-1] + 1,
                dp[i-1][j-1] + (ri != hj)
            )
    return dp[n][m] / max(1, n)

# --------------------------
# 평가 루프 (teacher-forced)
# --------------------------
@torch.no_grad()
def evaluate(model: CorrectionModel, loader: DataLoader, device) -> Dict[str, float]:
    model.eval()
    total_loss = 0.0
    total_tok  = 0
    total_cor  = 0
    total_cer  = 0.0
    n_cer      = 0

    dec_tok  = model.dec_tok
    pad_id   = dec_tok.pad_token_id
    eos_id   = dec_tok.eos_token_id

    for batch in tqdm(loader, desc="Validate/Test", leave=False):
        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}
        logits = model(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
            decoder_input_ids=batch["decoder_input_ids"],
            decoder_attention_mask=batch["decoder_attention_mask"],
            labels=None,
        )
        loss = criterion(logits, batch["labels"]).item()
        total_loss += loss

        preds = logits.argmax(-1)     # (B,T)
        golds = batch["labels"]       # (B,T)
        mask  = golds.ne(IGNORE_INDEX)
        total_tok += mask.sum().item()
        total_cor += (preds.eq(golds) & mask).sum().item()

        if EVAL_CER_MAX_SAMPLES == 0 or n_cer < EVAL_CER_MAX_SAMPLES:
            B = preds.size(0)
            for b in range(B):
                if EVAL_CER_MAX_SAMPLES and n_cer >= EVAL_CER_MAX_SAMPLES:
                    break
                hyp_text  = ids_to_text(dec_tok, preds[b], pad_id, eos_id)
                gold_text = ids_to_text(dec_tok, golds[b], pad_id, eos_id)
                total_cer += cer_one(gold_text, hyp_text)
                n_cer     += 1

    avg_loss = total_loss / max(1, len(loader))
    acc = (total_cor / max(1, total_tok)) if total_tok > 0 else 0.0
    avg_cer = (total_cer / max(1, n_cer)) if n_cer > 0 else 0.0
    return {"loss": avg_loss, "acc": acc, "cer": avg_cer}

# --------------------------
# 저장 유틸
# --------------------------
def save_ckpt(model: CorrectionModel, tag: str, step: int):
    path = os.path.join(CKPT_DIR, f"step_{step}_{tag}.pt")
    torch.save(model.state_dict(), path)
    print(f"[Save] {path}")

# --------------------------
# 메인 학습 루프
# --------------------------
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[Info] device: {device}")

    # steps/epoch 계산 (분할 개수 고정값 사용)
    steps_tr_ep = steps_per_epoch(TRAIN_LINES)
    total_steps = steps_tr_ep * EPOCHS
    print(f"[Split] train={TRAIN_LINES:,}, val={VAL_LINES:,}, test={TEST_LINES:,}")
    print(f"[Info] steps/epoch(train)={steps_tr_ep}, total_steps(est)={total_steps}")

    # 토크나이저/모델
    dec_tok = build_dec_tok(DECODER_SPM, legacy=False)
    print(f"[Info] Decoder vocab: {dec_tok.vocab_size}, PAD/BOS/EOS={dec_tok.pad_token_id}/{dec_tok.bos_token_id}/{dec_tok.eos_token_id}")

    model = build_model(dec_tok, device)
    set_encoder_trainable_layers(model, UNFREEZE_START_LAYERS)
    optimizer = build_optimizer(model)
    scheduler = build_scheduler(optimizer, total_steps)
    scaler = torch.amp.GradScaler(device="cuda", enabled=torch.cuda.is_available())

    # ✅ 분할 파일을 직접 읽는 DataLoader들
    def build_loader(ko_path, cor_path, bs=BATCH_SIZE, nw=NUM_WORKERS):
        ds = GrammarCorrectionDataset(
            ko_file=ko_path,
            correct_file=cor_path,
            tokenizer_name=ENCODER_NAME,
            max_length=ENC_MAX_LEN,
            stride=0,                      # IterableDataset에선 0 권장
            limit=None,                    # 파일 전부 사용
            decoder_tokenizer=dec_tok,
            dec_max_length=DEC_MAX_LEN,
            downsample_prob=1.0,
        )
        return DataLoader(ds, batch_size=bs, shuffle=False, num_workers=nw, pin_memory=PIN_MEMORY)

    train_loader = build_loader(KO_TRAIN_PATH, COR_TRAIN_PATH)
    val_loader   = build_loader(KO_VAL_PATH,   COR_VAL_PATH)
    test_loader  = build_loader(KO_TEST_PATH,  COR_TEST_PATH)

    state = TrainState()
    es    = ESState()

    for epoch in range(EPOCHS):
        # ===== Train =====
        model.train()
        pbar = tqdm(total=steps_tr_ep, desc=f"Epoch {epoch+1}/{EPOCHS} [train]", leave=True)
        accum = 0
        running_loss = 0.0
        running_steps = 0
        train_iter = iter(train_loader)

        while state.global_step // max(1, ACCUM_STEPS) < (epoch+1)*steps_tr_ep:
            try:
                batch = next(train_iter)
            except StopIteration:
                train_iter = iter(train_loader)
                batch = next(train_iter)

            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}
            with torch.amp.autocast(device_type="cuda", dtype=torch.float16, enabled=torch.cuda.is_available()):
                logits = model(
                    input_ids=batch["input_ids"],
                    attention_mask=batch["attention_mask"],
                    decoder_input_ids=batch["decoder_input_ids"],
                    decoder_attention_mask=batch["decoder_attention_mask"],
                    labels=None,
                )
                loss = criterion(logits, batch["labels"]) / ACCUM_STEPS

            scaler.scale(loss).backward()
            accum += 1

            if accum >= ACCUM_STEPS:
                if GRAD_CLIP_NORM and GRAD_CLIP_NORM > 0:
                    scaler.unscale_(optimizer)
                    nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad(set_to_none=True)
                scheduler.step()

                state.global_step += 1
                accum = 0

                l = loss.item() * ACCUM_STEPS
                running_loss += l
                running_steps += 1

                if state.ema_loss is None:
                    state.ema_loss = l
                else:
                    state.ema_loss = EMA_BETA * state.ema_loss + (1 - EMA_BETA) * l

                lr_now = optimizer.param_groups[2]["lr"]
                pbar.update(1)
                pbar.set_postfix({"loss": f"{l:.2f}", "ema": f"{state.ema_loss:.2f}", "lr": f"{lr_now:.2e}", "unfrz": state.unfrozen_layers})

                # (옵션) 스텝 저장
                if SAVE_EVERY_STEPS and state.global_step % SAVE_EVERY_STEPS == 0:
                    save_ckpt(model, "autosave", state.global_step)

                # plateau → 점진 언프리즈
                improved = (state.ema_loss + MIN_IMPROVE_RATIO * state.best_train_ema) < state.best_train_ema
                if improved:
                    state.best_train_ema = state.ema_loss
                    state.steps_since_improve = 0
                else:
                    state.steps_since_improve += 1
                    if state.steps_since_improve >= PATIENCE_STEPS_TRAIN and state.unfrozen_layers < UNFREEZE_MAX_LAYERS:
                        new_layers = min(UNFREEZE_MAX_LAYERS, state.unfrozen_layers + UNFREEZE_STEP_LAYERS)
                        print(f"[Unfreeze] plateau → encoder last {UNFREEZE_STEP_LAYERS} layers: {state.unfrozen_layers} → {new_layers}")
                        state.unfrozen_layers = new_layers
                        set_encoder_trainable_layers(model, state.unfrozen_layers)
                        state.steps_since_improve = 0

        pbar.close()

        train_avg_loss = running_loss / max(1, running_steps)
        lr_now = optimizer.param_groups[2]["lr"]
        print(f"[train] epoch {epoch+1} | avg_loss {train_avg_loss:.4f} | ema {state.ema_loss:.4f} | lr {lr_now:.2e} | unfrz {state.unfrozen_layers}")

        # ===== Validate =====
        val_metrics = evaluate(model, val_loader, device)
        print(f"[val] epoch {epoch+1} | loss {val_metrics['loss']:.4f} | acc {val_metrics['acc']*100:.2f}% | cer {val_metrics['cer']:.4f}")

        # 에폭 저장 + 마일스톤 저장
        save_ckpt(model, f"epoch{epoch+1}", state.global_step)
        if (epoch + 1) % MILESTONE_EVERY_EPOCHS == 0:
            save_ckpt(model, f"epoch{epoch+1}_milestone", state.global_step)

        # ===== Early Stopping =====
        monitor_value = val_metrics["loss"] if ES_MONITOR == "val_loss" else val_metrics["acc"]
        better = (monitor_value + (ES_MIN_DELTA if ES_MONITOR=="val_loss" else -ES_MIN_DELTA))
        if (ES_MONITOR == "val_loss" and better < es.best_metric) or (ES_MONITOR != "val_loss" and better > es.best_metric):
            es.best_metric = monitor_value
            es.best_epoch = epoch + 1
            es.wait = 0
            save_ckpt(model, f"best_{ES_MONITOR}", state.global_step)
        else:
            es.wait += 1
            if es.wait >= ES_PATIENCE_EP:
                print(f"[EarlyStop] No improvement in {ES_MONITOR} for {ES_PATIENCE_EP} epochs (best at epoch {es.best_epoch}).")
                break

    # ===== Test =====
    test_metrics = evaluate(model, test_loader, device)
    print(f"[test] loss {test_metrics['loss']:.4f} | acc {test_metrics['acc']*100:.2f}% | cer {test_metrics['cer']:.4f}")
    print("[Done]")

if __name__ == "__main__":
    main()
